{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pltf The next generation of Infrastructure-as-Code: work with high-level constructs instead of getting lost in low-level cloud configuration. pltf is a higher-level Infrastructure-as-Code framework. Instead of hand-crafting low-level cloud config, you describe environments and services in concise YAML. pltf turns those high-level constructs into Terraform so you keep full portability\u2014generate the code, extend it, or take it with you. Why pltf Infrastructure-as-code is essential, but working directly with low-level cloud and Terraform can be complex. pltf bakes in cloud/IaC best practices so you can set up automated, scalable, and secure infrastructure quickly\u2014without being a full-time DevOps engineer. Because pltf emits Terraform, you avoid lock-in and can extend or own the generated code at any time. How It Works With pltf you write configuration files and run the CLI (locally or in CI/CD). The CLI connects to your cloud, renders Terraform (providers/backends/locals/remote state), and can execute Terraform for you. Status: active development, not yet production-hardened. Pin versions and review generated code before applying. There are two primary spec types: Environment : defines cloud/provider, account, region, backend, and shared modules (clusters, networks, IAM, ingress, etc.). You might have one per staging/prod/QA, or per engineer/PR for isolated sandboxes. Service : defines an application workload and the non-Kubernetes resources it needs, linked to an Environment. Service specs seamlessly connect to environment outputs and modules. Environment and service specs are linked via metadata.ref and envRef . What You Can Do Generate IaC fast : turn Environment/Service YAML into Terraform with consistent providers/backends/remote state. Mix modules : use the embedded catalog or your own ( source: custom ) with the same wiring rules. Choose backends : store state in s3|gcs|azurerm regardless of target cloud; use profiles for cross-account S3. Run Terraform safely : pltf terraform plan/apply/destroy/output/unlock auto-generate before executing TF. Validate & lint : structural checks plus suggestions (labels, unused vars). Preview : see provider/backend/labels/modules without running TF. Next Steps Follow Getting Started . Explore repo examples ( example/env.yaml , example/service.yaml ). Review Security . Quick Links Installation Getting Started Platform Usage CLI Reference Spec Guide Modules & Wiring Features References Security","title":"Home"},{"location":"#pltf","text":"The next generation of Infrastructure-as-Code: work with high-level constructs instead of getting lost in low-level cloud configuration. pltf is a higher-level Infrastructure-as-Code framework. Instead of hand-crafting low-level cloud config, you describe environments and services in concise YAML. pltf turns those high-level constructs into Terraform so you keep full portability\u2014generate the code, extend it, or take it with you.","title":"pltf"},{"location":"#why-pltf","text":"Infrastructure-as-code is essential, but working directly with low-level cloud and Terraform can be complex. pltf bakes in cloud/IaC best practices so you can set up automated, scalable, and secure infrastructure quickly\u2014without being a full-time DevOps engineer. Because pltf emits Terraform, you avoid lock-in and can extend or own the generated code at any time.","title":"Why pltf"},{"location":"#how-it-works","text":"With pltf you write configuration files and run the CLI (locally or in CI/CD). The CLI connects to your cloud, renders Terraform (providers/backends/locals/remote state), and can execute Terraform for you. Status: active development, not yet production-hardened. Pin versions and review generated code before applying. There are two primary spec types: Environment : defines cloud/provider, account, region, backend, and shared modules (clusters, networks, IAM, ingress, etc.). You might have one per staging/prod/QA, or per engineer/PR for isolated sandboxes. Service : defines an application workload and the non-Kubernetes resources it needs, linked to an Environment. Service specs seamlessly connect to environment outputs and modules. Environment and service specs are linked via metadata.ref and envRef .","title":"How It Works"},{"location":"#what-you-can-do","text":"Generate IaC fast : turn Environment/Service YAML into Terraform with consistent providers/backends/remote state. Mix modules : use the embedded catalog or your own ( source: custom ) with the same wiring rules. Choose backends : store state in s3|gcs|azurerm regardless of target cloud; use profiles for cross-account S3. Run Terraform safely : pltf terraform plan/apply/destroy/output/unlock auto-generate before executing TF. Validate & lint : structural checks plus suggestions (labels, unused vars). Preview : see provider/backend/labels/modules without running TF.","title":"What You Can Do"},{"location":"#next-steps","text":"Follow Getting Started . Explore repo examples ( example/env.yaml , example/service.yaml ). Review Security .","title":"Next Steps"},{"location":"#quick-links","text":"Installation Getting Started Platform Usage CLI Reference Spec Guide Modules & Wiring Features References Security","title":"Quick Links"},{"location":"faq/","text":"FAQ Placeholder content. Replace with real Q&A. What is pltf? A CLI that turns YAML specs into Terraform. Can I use custom modules? Yes, set source: custom and provide a custom modules root. How do I pick a backend? Set backend.type to s3|gcs|azurerm regardless of provider.","title":"FAQ"},{"location":"faq/#faq","text":"Placeholder content. Replace with real Q&A. What is pltf? A CLI that turns YAML specs into Terraform. Can I use custom modules? Yes, set source: custom and provide a custom modules root. How do I pick a backend? Set backend.type to s3|gcs|azurerm regardless of provider.","title":"FAQ"},{"location":"features/","text":"Features Overview Each major capability has its own page: Profiles & Defaults Validation & Lint Backends Custom Modules Placeholders & Wiring Secrets Variables Telemetry Secrets What: Manage app secrets without embedding them in specs/code. Secrets are stored as Kubernetes secrets and injected as env vars. Why: Avoid leaking credentials; keep rotation simple. Usage: Define secret keys in your spec under secrets and supply values via environment variables or CLI --var . Secrets are treated as TF variables, not locals. Notes: Services restart to pick up changes unless --no-restart is used. Bulk updates can consume .env -style inputs; values should come from env/CI secret stores, not hardcoded files. Terraform Generator What: Render Terraform from env/service specs without applying; handy for review, migration, or running TF directly. Why: Keep portability\u2014inspect/modify TF, hand to CI, or migrate away without lock-in. Commands: pltf generate for TF only; pltf terraform plan|apply|destroy|output|force-unlock to generate + run. Example (env): bash pltf generate -f env.yaml -e prod -o .pltf/env/prod # outputs providers.tf, backend.tf, modules/<...>, outputs.tf, versions.tf Example (service): bash pltf generate -f service.yaml -e prod -o .pltf/service/payments/prod Notes: Does not require cloud credentials to render. Backends are written per spec ( s3|gcs|azurerm ). Generated modules directory is self-contained for review or VCS. Variables What: Minimal templating to reuse specs across envs/services. Types: CLI --var , env-level variables , and placeholders. Placeholders: ${env_name} , ${layer_name} , ${module.<id>.<output>} , ${parent.<output>} , ${var.<name>} . Spec inputs: Declare variables in env specs or use --var key=value at runtime; service specs inherit envRef variables and can override via CLI. Example (env): ```yaml variables: min_nodes: \"2\" max_nodes: \"5\" modules: type: aws_eks min_nodes: \"${var.min_nodes}\" max_nodes: \"${var.max_nodes}\" bash pltf terraform apply -f env.yaml -e prod --var min_nodes=3 --var max_nodes=6 ``` Parent outputs: In services, ${parent.<output>} references environment outputs (e.g., ${parent.domain} ).","title":"Overview"},{"location":"features/#features-overview","text":"Each major capability has its own page: Profiles & Defaults Validation & Lint Backends Custom Modules Placeholders & Wiring Secrets Variables Telemetry","title":"Features Overview"},{"location":"features/#secrets","text":"What: Manage app secrets without embedding them in specs/code. Secrets are stored as Kubernetes secrets and injected as env vars. Why: Avoid leaking credentials; keep rotation simple. Usage: Define secret keys in your spec under secrets and supply values via environment variables or CLI --var . Secrets are treated as TF variables, not locals. Notes: Services restart to pick up changes unless --no-restart is used. Bulk updates can consume .env -style inputs; values should come from env/CI secret stores, not hardcoded files.","title":"Secrets"},{"location":"features/#terraform-generator","text":"What: Render Terraform from env/service specs without applying; handy for review, migration, or running TF directly. Why: Keep portability\u2014inspect/modify TF, hand to CI, or migrate away without lock-in. Commands: pltf generate for TF only; pltf terraform plan|apply|destroy|output|force-unlock to generate + run. Example (env): bash pltf generate -f env.yaml -e prod -o .pltf/env/prod # outputs providers.tf, backend.tf, modules/<...>, outputs.tf, versions.tf Example (service): bash pltf generate -f service.yaml -e prod -o .pltf/service/payments/prod Notes: Does not require cloud credentials to render. Backends are written per spec ( s3|gcs|azurerm ). Generated modules directory is self-contained for review or VCS.","title":"Terraform Generator"},{"location":"features/#variables","text":"What: Minimal templating to reuse specs across envs/services. Types: CLI --var , env-level variables , and placeholders. Placeholders: ${env_name} , ${layer_name} , ${module.<id>.<output>} , ${parent.<output>} , ${var.<name>} . Spec inputs: Declare variables in env specs or use --var key=value at runtime; service specs inherit envRef variables and can override via CLI. Example (env): ```yaml variables: min_nodes: \"2\" max_nodes: \"5\" modules: type: aws_eks min_nodes: \"${var.min_nodes}\" max_nodes: \"${var.max_nodes}\" bash pltf terraform apply -f env.yaml -e prod --var min_nodes=3 --var max_nodes=6 ``` Parent outputs: In services, ${parent.<output>} references environment outputs (e.g., ${parent.domain} ).","title":"Variables"},{"location":"installation/","text":"Installation Homebrew (macOS/Linux) brew tap yindia/pltf brew install pltf If you previously tapped another repo, run brew untap <old> before tapping yindia/pltf . Install script (macOS/Linux/Windows via WSL or Git Bash) curl -sSL https://raw.githubusercontent.com/yindia/pltf/main/scripts/install.sh | sh Environment overrides: - REPO_OWNER / REPO_NAME to point at a fork - VERSION to pin a release tag (e.g. v1.2.3 ) - DEST to change install path (defaults to /usr/local/bin ) Docker docker build -t pltf . docker run --rm pltf --help From source go install ./... # or to build a local binary go build -o bin/pltf main.go Verify pltf --help pltf validate -f env.yaml Upgrade Homebrew: brew upgrade pltf Script: rerun the install script with the desired VERSION From source: git pull then rebuild","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#homebrew-macoslinux","text":"brew tap yindia/pltf brew install pltf If you previously tapped another repo, run brew untap <old> before tapping yindia/pltf .","title":"Homebrew (macOS/Linux)"},{"location":"installation/#install-script-macoslinuxwindows-via-wsl-or-git-bash","text":"curl -sSL https://raw.githubusercontent.com/yindia/pltf/main/scripts/install.sh | sh Environment overrides: - REPO_OWNER / REPO_NAME to point at a fork - VERSION to pin a release tag (e.g. v1.2.3 ) - DEST to change install path (defaults to /usr/local/bin )","title":"Install script (macOS/Linux/Windows via WSL or Git Bash)"},{"location":"installation/#docker","text":"docker build -t pltf . docker run --rm pltf --help","title":"Docker"},{"location":"installation/#from-source","text":"go install ./... # or to build a local binary go build -o bin/pltf main.go","title":"From source"},{"location":"installation/#verify","text":"pltf --help pltf validate -f env.yaml","title":"Verify"},{"location":"installation/#upgrade","text":"Homebrew: brew upgrade pltf Script: rerun the install script with the desired VERSION From source: git pull then rebuild","title":"Upgrade"},{"location":"modules/","text":"Modules & Wiring Modules are discovered from a modules root where each module type directory contains a module.yaml . The CLI scans custom roots (when provided) and the embedded catalog. Modules marked source: custom must be found in your custom root; others fall back to embedded. Wiring rules Inputs auto-wire to outputs with the same name (current scope, or parent env for services). Required inputs without a value or matching output fail validation. Optional/default inputs can stay unwired if nothing matches. Templates ${module.*} , ${var.*} , ${parent.*} are supported and converted to Terraform traversals. Module metadata (module.yaml) Example fields: name: aws_eks type: aws_eks provider: aws version: 1.0.0 description: EKS cluster inputs: - name: cluster_name type: string required: true - name: enable_metrics type: bool required: true - name: env_name type: string required: true outputs: - name: k8s_cluster_name type: string Notes: - Inputs may include description , default , capability (optional). - Outputs may include description , capability . - Capabilities can declare provides / accepts to describe contracts. Embedded modules (AWS) aws_base , aws_dns , aws_eks , aws_k8s_base , aws_k8s_service , aws_nodegroup aws_postgres , aws_mysql , aws_redis , aws_dynamodb , aws_s3 , aws_ses , aws_sns , aws_sqs , aws_documentdb aws_iam_role , aws_iam_policy , aws_iam_user cloudfront_distribution GCP/Azure: no bundled modules yet; use custom modules or your own registry. You can target GCP/Azure providers with custom modules and backends. Custom modules Mark spec entries with source: custom to force lookup in your custom modules root ( --modules or profile modules_root ). Generate module.yaml for your module with pltf module init --path <module_dir> [--force] . Inventory commands: pltf module list|get [-m ./modules] -o table|json|yaml . Treat modules as black boxes: configure via inputs , consume declared outputs , and let wiring handle references. Module init helper Use pltf module init --path <module_dir> [--force] to generate or refresh module.yaml from an existing Terraform module. This inspects variables/outputs and writes a fresh descriptor (backing up or overwriting if --force ).","title":"Modules & Wiring"},{"location":"modules/#modules-wiring","text":"Modules are discovered from a modules root where each module type directory contains a module.yaml . The CLI scans custom roots (when provided) and the embedded catalog. Modules marked source: custom must be found in your custom root; others fall back to embedded.","title":"Modules &amp; Wiring"},{"location":"modules/#wiring-rules","text":"Inputs auto-wire to outputs with the same name (current scope, or parent env for services). Required inputs without a value or matching output fail validation. Optional/default inputs can stay unwired if nothing matches. Templates ${module.*} , ${var.*} , ${parent.*} are supported and converted to Terraform traversals.","title":"Wiring rules"},{"location":"modules/#module-metadata-moduleyaml","text":"Example fields: name: aws_eks type: aws_eks provider: aws version: 1.0.0 description: EKS cluster inputs: - name: cluster_name type: string required: true - name: enable_metrics type: bool required: true - name: env_name type: string required: true outputs: - name: k8s_cluster_name type: string Notes: - Inputs may include description , default , capability (optional). - Outputs may include description , capability . - Capabilities can declare provides / accepts to describe contracts.","title":"Module metadata (module.yaml)"},{"location":"modules/#embedded-modules-aws","text":"aws_base , aws_dns , aws_eks , aws_k8s_base , aws_k8s_service , aws_nodegroup aws_postgres , aws_mysql , aws_redis , aws_dynamodb , aws_s3 , aws_ses , aws_sns , aws_sqs , aws_documentdb aws_iam_role , aws_iam_policy , aws_iam_user cloudfront_distribution GCP/Azure: no bundled modules yet; use custom modules or your own registry. You can target GCP/Azure providers with custom modules and backends.","title":"Embedded modules (AWS)"},{"location":"modules/#custom-modules","text":"Mark spec entries with source: custom to force lookup in your custom modules root ( --modules or profile modules_root ). Generate module.yaml for your module with pltf module init --path <module_dir> [--force] . Inventory commands: pltf module list|get [-m ./modules] -o table|json|yaml . Treat modules as black boxes: configure via inputs , consume declared outputs , and let wiring handle references.","title":"Custom modules"},{"location":"modules/#module-init-helper","text":"Use pltf module init --path <module_dir> [--force] to generate or refresh module.yaml from an existing Terraform module. This inspects variables/outputs and writes a fresh descriptor (backing up or overwriting if --force ).","title":"Module init helper"},{"location":"platform/","text":"Platform Usage Use this page as a practical guide to the most common flows in pltf. Validate + Lint pltf validate -f env.yaml -e prod pltf validate -f service.yaml -e dev Runs structural validation and lint suggestions (labels, unused vars). Picks environment from --env , PLTF_DEFAULT_ENV , or profile default_env . Preview pltf preview -f env.yaml -e prod Shows provider, backend type, labels, and modules without running Terraform. Generate (Terraform only) pltf generate -f env.yaml -e dev pltf generate -f service.yaml -e prod -m ./modules --out .pltf/service/prod pltf generate -f service.yaml -e dev --var cluster_name=my-dev --modules/-m custom root; modules with source: custom resolve here first. --out/-o defaults to .pltf/<env_name>/env/<env> or .pltf/<env_name>/<service>/<env> . --var/-v merges over env vars \u2192 service envRef vars \u2192 CLI vars. File inputs pointing to existing files in the spec directory are copied into the output and paths are updated. Terraform commands pltf terraform plan -f service.yaml -e dev # supports --target, --parallelism, --detailed-exitcode, --plan-file pltf terraform apply -f env.yaml -e prod pltf terraform destroy -f env.yaml -e prod pltf terraform output -f service.yaml -e dev --json pltf terraform force-unlock -f env.yaml -e prod --lock-id=<id> Automatically generates Terraform, ensures backend (S3/GCS/Azurerm). Common flags: --target/-t , --parallelism/-p , --lock/-l , --lock-timeout/-T , --no-color/-C , --input/-i , --refresh/-r , --plan-file/-P , --detailed-exitcode/-d , --json/-j . Module inventory pltf module list [-m ./modules] [-o table|json|yaml] pltf module get aws_eks [-m ./modules] [-o table|json|yaml] pltf module init --path ./modules/aws_eks [--force] Use source: custom in specs to force lookup from your custom root ( --modules or profile modules_root ); embedded modules remain available. Profiles & Defaults ~/.pltf/profile.yaml (or PLTF_PROFILE ) can set modules_root , default_env , default_out , telemetry . PLTF_DEFAULT_ENV is also respected for picking the environment. Backends backend.type can be s3|gcs|azurerm (independent of provider). backend.profile supports cross-account S3; optional region , container , resource_group .","title":"Platform Usage"},{"location":"platform/#platform-usage","text":"Use this page as a practical guide to the most common flows in pltf.","title":"Platform Usage"},{"location":"platform/#validate-lint","text":"pltf validate -f env.yaml -e prod pltf validate -f service.yaml -e dev Runs structural validation and lint suggestions (labels, unused vars). Picks environment from --env , PLTF_DEFAULT_ENV , or profile default_env .","title":"Validate + Lint"},{"location":"platform/#preview","text":"pltf preview -f env.yaml -e prod Shows provider, backend type, labels, and modules without running Terraform.","title":"Preview"},{"location":"platform/#generate-terraform-only","text":"pltf generate -f env.yaml -e dev pltf generate -f service.yaml -e prod -m ./modules --out .pltf/service/prod pltf generate -f service.yaml -e dev --var cluster_name=my-dev --modules/-m custom root; modules with source: custom resolve here first. --out/-o defaults to .pltf/<env_name>/env/<env> or .pltf/<env_name>/<service>/<env> . --var/-v merges over env vars \u2192 service envRef vars \u2192 CLI vars. File inputs pointing to existing files in the spec directory are copied into the output and paths are updated.","title":"Generate (Terraform only)"},{"location":"platform/#terraform-commands","text":"pltf terraform plan -f service.yaml -e dev # supports --target, --parallelism, --detailed-exitcode, --plan-file pltf terraform apply -f env.yaml -e prod pltf terraform destroy -f env.yaml -e prod pltf terraform output -f service.yaml -e dev --json pltf terraform force-unlock -f env.yaml -e prod --lock-id=<id> Automatically generates Terraform, ensures backend (S3/GCS/Azurerm). Common flags: --target/-t , --parallelism/-p , --lock/-l , --lock-timeout/-T , --no-color/-C , --input/-i , --refresh/-r , --plan-file/-P , --detailed-exitcode/-d , --json/-j .","title":"Terraform commands"},{"location":"platform/#module-inventory","text":"pltf module list [-m ./modules] [-o table|json|yaml] pltf module get aws_eks [-m ./modules] [-o table|json|yaml] pltf module init --path ./modules/aws_eks [--force] Use source: custom in specs to force lookup from your custom root ( --modules or profile modules_root ); embedded modules remain available.","title":"Module inventory"},{"location":"platform/#profiles-defaults","text":"~/.pltf/profile.yaml (or PLTF_PROFILE ) can set modules_root , default_env , default_out , telemetry . PLTF_DEFAULT_ENV is also respected for picking the environment.","title":"Profiles &amp; Defaults"},{"location":"platform/#backends","text":"backend.type can be s3|gcs|azurerm (independent of provider). backend.profile supports cross-account S3; optional region , container , resource_group .","title":"Backends"},{"location":"specs/","text":"Spec Guide pltf reads YAML specs with kind: Environment or kind: Service . The CLI validates structure and wires modules based on names and templated references. Environment spec (kind: Environment) Minimal shape: apiVersion: platform.io/v1 kind: Environment metadata: name: example-aws org: example-org provider: aws labels: team: platform backend: type: s3 bucket: example-tfstate # optional; auto-named if omitted region: us-east-1 environments: dev: account: \"111111111111\" region: us-east-1 variables: base_domain: dev.example.com secrets: db_password: {} modules: - id: base type: aws_base - id: dns type: aws_dns inputs: domain: var.base_domain Notes: - environments map holds per-env accounts/regions/vars/secrets. - modules list holds shared modules; id / type required; inputs optional; links supported. - Backend: backend.type can be s3|gcs|azurerm (independent of provider). backend.profile supports cross-account S3; container/resource_group for azurerm. - Modules can set source: custom to force resolution from your custom modules root ( --modules or profile modules_root ); others fall back to the embedded catalog. Service spec (kind: Service) Minimal shape: apiVersion: platform.io/v1 kind: Service metadata: name: payments-api ref: ./env.yaml # path to Environment spec envRef: dev: variables: cluster_name: dev-cluster modules: - id: app type: aws_k8s_service inputs: cluster_name: var.cluster_name public_uri: \"/payments\" image: \"ghcr.io/acme/payments:latest\" links: readwrite: - db - id: db type: aws_postgres Notes: - metadata.ref points to the Environment file (relative paths allowed). - envRef holds per-env variables/secrets merged after environment variables. - Modules can reference environment outputs via ${parent.<output>} . Variable precedence 1) Environment variables 2) Service envRef variables (service only) 3) CLI --var key=value Secrets vs locals Secrets remain as Terraform variables ( var.<name> ). Non-secrets become locals; var.<name> resolves to locals unless marked secret. Templated references ${module.<module>.<output>} \u2014 module output in current scope ${var.<name>} \u2014 logical variable; wires to locals/secrets when names match ${parent.<output>} \u2014 environment output via remote state (service only) ${env_name} / ${layer_name} \u2014 intrinsic placeholders; for services, layer_name is the service name","title":"Spec Guide"},{"location":"specs/#spec-guide","text":"pltf reads YAML specs with kind: Environment or kind: Service . The CLI validates structure and wires modules based on names and templated references.","title":"Spec Guide"},{"location":"specs/#environment-spec-kind-environment","text":"Minimal shape: apiVersion: platform.io/v1 kind: Environment metadata: name: example-aws org: example-org provider: aws labels: team: platform backend: type: s3 bucket: example-tfstate # optional; auto-named if omitted region: us-east-1 environments: dev: account: \"111111111111\" region: us-east-1 variables: base_domain: dev.example.com secrets: db_password: {} modules: - id: base type: aws_base - id: dns type: aws_dns inputs: domain: var.base_domain Notes: - environments map holds per-env accounts/regions/vars/secrets. - modules list holds shared modules; id / type required; inputs optional; links supported. - Backend: backend.type can be s3|gcs|azurerm (independent of provider). backend.profile supports cross-account S3; container/resource_group for azurerm. - Modules can set source: custom to force resolution from your custom modules root ( --modules or profile modules_root ); others fall back to the embedded catalog.","title":"Environment spec (kind: Environment)"},{"location":"specs/#service-spec-kind-service","text":"Minimal shape: apiVersion: platform.io/v1 kind: Service metadata: name: payments-api ref: ./env.yaml # path to Environment spec envRef: dev: variables: cluster_name: dev-cluster modules: - id: app type: aws_k8s_service inputs: cluster_name: var.cluster_name public_uri: \"/payments\" image: \"ghcr.io/acme/payments:latest\" links: readwrite: - db - id: db type: aws_postgres Notes: - metadata.ref points to the Environment file (relative paths allowed). - envRef holds per-env variables/secrets merged after environment variables. - Modules can reference environment outputs via ${parent.<output>} .","title":"Service spec (kind: Service)"},{"location":"specs/#variable-precedence","text":"1) Environment variables 2) Service envRef variables (service only) 3) CLI --var key=value","title":"Variable precedence"},{"location":"specs/#secrets-vs-locals","text":"Secrets remain as Terraform variables ( var.<name> ). Non-secrets become locals; var.<name> resolves to locals unless marked secret.","title":"Secrets vs locals"},{"location":"specs/#templated-references","text":"${module.<module>.<output>} \u2014 module output in current scope ${var.<name>} \u2014 logical variable; wires to locals/secrets when names match ${parent.<output>} \u2014 environment output via remote state (service only) ${env_name} / ${layer_name} \u2014 intrinsic placeholders; for services, layer_name is the service name","title":"Templated references"},{"location":"usage/","text":"CLI Usage pltf auto-detects whether a spec is an Environment or Service based on kind . Most commands accept --file/-f , --env/-e , --modules/-m , --out/-o , and --var/-v key=value . Profiles ( ~/.pltf/profile.yaml or PLTF_PROFILE ) can set defaults for modules_root and default_env . Command catalog pltf validate \u2014 validate + lint specs. pltf generate \u2014 render Terraform only. pltf preview \u2014 summarize provider/backend/labels/modules. pltf terraform plan|apply|destroy|output|force-unlock|graph \u2014 generate + run Terraform with standard TF flags. pltf module list|get|init \u2014 module inventory and metadata generation. pltf lint \u2014 lint only (also run implicitly by validate). validate What: Validate Environment or Service specs; auto-detects kind; runs lint suggestions (labels, unused vars). Flags: --file/-f \u2014 Path to the spec (default env.yaml ). --env/-e \u2014 Environment key (dev/prod/etc.). Example: pltf validate -f service.yaml -e dev generate What: Render Terraform only; no init/apply. Flags: --file/-f \u2014 Path to the spec. --env/-e \u2014 Environment key. --modules/-m \u2014 Custom modules root; source: custom resolves here first. --out/-o \u2014 Output dir (defaults to .pltf/<env_name>/env/<env> or .pltf/<env_name>/<service>/<env> ). --var/-v \u2014 CLI var override key=value . Example: pltf generate -f service.yaml -e prod -m ./modules --out .pltf/service/prod preview What: Show provider, backend, labels, modules without running Terraform. Flags: --file/-f , --env/-e Example: pltf preview -f env.yaml -e prod terraform plan What: Generate + run terraform plan with standard flags. Plan flags: --target/-t \u2014 Target address (repeatable). --parallelism/-p \u2014 Max parallel operations. --lock/-l \u2014 Lock state (default true). --lock-timeout/-T \u2014 Lock timeout (e.g., 30s). --no-color/-C \u2014 Disable color output. --input/-i \u2014 Prompt for input (default false). --refresh/-r \u2014 Refresh state before plan (default true). --detailed-exitcode/-d \u2014 Enable TF detailed exit codes. --plan-file/-P \u2014 Write plan to a file. Shared flags: --file/-f , --env/-e , --modules/-m , --out/-o , --var/-v Example: pltf terraform plan -f service.yaml -e dev --detailed-exitcode --plan-file=/tmp/plan.tfplan terraform apply What: Generate + run terraform apply -auto-approve . Flags: Shared flags ( --file/-f , --env/-e , --modules/-m , --out/-o , --var/-v ). Example: pltf terraform apply -f env.yaml -e prod terraform destroy What: Generate (if needed) + run terraform destroy -auto-approve . Flags: Same as apply. Example: pltf terraform destroy -f env.yaml -e prod terraform output What: Show outputs (optionally JSON). Flags: --json/-j (JSON output), plus shared --file/-f , --env/-e , --modules/-m , --out/-o Example: pltf terraform output -f service.yaml -e dev --json terraform force-unlock What: Force unlock state. Flags: --lock-id (required), plus shared --file/-f , --env/-e , --modules/-m , --out/-o Example: pltf terraform force-unlock -f env.yaml -e prod --lock-id=12345 terraform graph What: Emit DOT graph. Default runs terraform graph ; --mode spec renders a dependency graph from the YAML only. Flags: --mode terraform|spec (default terraform), --plan-file/-P (passed to terraform graph), plus shared --file/-f , --env/-e , --modules/-m , --out/-o Example: pltf terraform graph -f env.yaml -e dev | dot -Tpng > graph.png module list What: List module inventory from embedded/custom roots. Flags: --modules/-m (modules root), --output/-o ( table|json|yaml ) Example: pltf module list -m ./modules -o json module get What: Show module details (inputs/outputs). Flags: Same as module list. Example: pltf module get aws_eks -m ./modules module init What: Generate module.yaml from an existing Terraform module dir. Flags: --path (module dir), --name , --type , --description , --out , --force (overwrite) Example: pltf module init --path ./modules/aws_eks --force Validate + Lint Structural validation plus lint suggestions (labels, unused vars). pltf validate -f env.yaml -e prod pltf validate -f service.yaml -e dev Generate Render Terraform without running it. File inputs that point to existing files (relative to the spec) are copied into the output directory and paths are updated. pltf generate -f env.yaml -e dev pltf generate -f service.yaml -e prod -o .pltf/service/prod pltf generate -f service.yaml -e dev -m ./custom-mods --var cluster_name=my-dev Flags: - --modules/-m custom modules root. Modules with source: custom are resolved only from the custom root; others fall back to embedded modules. - --out/-o output dir (defaults .pltf/<env_name>/env/<env> or .pltf/<env_name>/<service>/<env> ). - --var/-v merges vars (env vars \u2192 service envRef vars \u2192 CLI vars). Terraform helpers Terraform commands live under pltf terraform ... and auto-generate before running TF. pltf terraform plan -f service.yaml -e dev # plan (supports --target, --parallelism, --detailed-exitcode, --plan-file) pltf terraform apply -f env.yaml -e prod # apply pltf terraform destroy -f env.yaml -e prod # destroy pltf terraform output -f service.yaml -e dev # outputs (--json supported) pltf terraform force-unlock -f env.yaml -e prod --lock-id=<id> Common flags: --target/-t , --parallelism/-p , --lock/-l , --lock-timeout/-T , --no-color/-C , --input/-i , --refresh/-r , --plan-file/-P , --detailed-exitcode/-d , --json/-j . Preview Quick summary (provider, backend, labels, modules) without TF. pltf preview -f env.yaml -e prod Module inventory pltf module list [-m ./modules] [-o table|json|yaml] pltf module get aws_eks [-m ./modules] [-o table|json|yaml] pltf module init --path ./modules/aws_eks [--force] Custom backends In env spec backend.type can be s3|gcs|azurerm regardless of provider. Optional region , container , resource_group , profile (S3) supported. Custom modules Mark a module with source: custom to force lookup in your custom modules root. Provide a custom root via --modules or profile modules_root ; embedded modules remain available for everything else. Generate module.yaml for your own TF module with pltf module init --path <module_dir> [--force] . Environment defaults PLTF_DEFAULT_ENV or profile default_env let you omit --env when only one environment applies. Completions pltf completion bash|zsh|fish|powershell","title":"CLI Reference"},{"location":"usage/#cli-usage","text":"pltf auto-detects whether a spec is an Environment or Service based on kind . Most commands accept --file/-f , --env/-e , --modules/-m , --out/-o , and --var/-v key=value . Profiles ( ~/.pltf/profile.yaml or PLTF_PROFILE ) can set defaults for modules_root and default_env .","title":"CLI Usage"},{"location":"usage/#command-catalog","text":"pltf validate \u2014 validate + lint specs. pltf generate \u2014 render Terraform only. pltf preview \u2014 summarize provider/backend/labels/modules. pltf terraform plan|apply|destroy|output|force-unlock|graph \u2014 generate + run Terraform with standard TF flags. pltf module list|get|init \u2014 module inventory and metadata generation. pltf lint \u2014 lint only (also run implicitly by validate).","title":"Command catalog"},{"location":"usage/#validate","text":"What: Validate Environment or Service specs; auto-detects kind; runs lint suggestions (labels, unused vars). Flags: --file/-f \u2014 Path to the spec (default env.yaml ). --env/-e \u2014 Environment key (dev/prod/etc.). Example: pltf validate -f service.yaml -e dev","title":"validate"},{"location":"usage/#generate","text":"What: Render Terraform only; no init/apply. Flags: --file/-f \u2014 Path to the spec. --env/-e \u2014 Environment key. --modules/-m \u2014 Custom modules root; source: custom resolves here first. --out/-o \u2014 Output dir (defaults to .pltf/<env_name>/env/<env> or .pltf/<env_name>/<service>/<env> ). --var/-v \u2014 CLI var override key=value . Example: pltf generate -f service.yaml -e prod -m ./modules --out .pltf/service/prod","title":"generate"},{"location":"usage/#preview","text":"What: Show provider, backend, labels, modules without running Terraform. Flags: --file/-f , --env/-e Example: pltf preview -f env.yaml -e prod","title":"preview"},{"location":"usage/#terraform-plan","text":"What: Generate + run terraform plan with standard flags. Plan flags: --target/-t \u2014 Target address (repeatable). --parallelism/-p \u2014 Max parallel operations. --lock/-l \u2014 Lock state (default true). --lock-timeout/-T \u2014 Lock timeout (e.g., 30s). --no-color/-C \u2014 Disable color output. --input/-i \u2014 Prompt for input (default false). --refresh/-r \u2014 Refresh state before plan (default true). --detailed-exitcode/-d \u2014 Enable TF detailed exit codes. --plan-file/-P \u2014 Write plan to a file. Shared flags: --file/-f , --env/-e , --modules/-m , --out/-o , --var/-v Example: pltf terraform plan -f service.yaml -e dev --detailed-exitcode --plan-file=/tmp/plan.tfplan","title":"terraform plan"},{"location":"usage/#terraform-apply","text":"What: Generate + run terraform apply -auto-approve . Flags: Shared flags ( --file/-f , --env/-e , --modules/-m , --out/-o , --var/-v ). Example: pltf terraform apply -f env.yaml -e prod","title":"terraform apply"},{"location":"usage/#terraform-destroy","text":"What: Generate (if needed) + run terraform destroy -auto-approve . Flags: Same as apply. Example: pltf terraform destroy -f env.yaml -e prod","title":"terraform destroy"},{"location":"usage/#terraform-output","text":"What: Show outputs (optionally JSON). Flags: --json/-j (JSON output), plus shared --file/-f , --env/-e , --modules/-m , --out/-o Example: pltf terraform output -f service.yaml -e dev --json","title":"terraform output"},{"location":"usage/#terraform-force-unlock","text":"What: Force unlock state. Flags: --lock-id (required), plus shared --file/-f , --env/-e , --modules/-m , --out/-o Example: pltf terraform force-unlock -f env.yaml -e prod --lock-id=12345","title":"terraform force-unlock"},{"location":"usage/#terraform-graph","text":"What: Emit DOT graph. Default runs terraform graph ; --mode spec renders a dependency graph from the YAML only. Flags: --mode terraform|spec (default terraform), --plan-file/-P (passed to terraform graph), plus shared --file/-f , --env/-e , --modules/-m , --out/-o Example: pltf terraform graph -f env.yaml -e dev | dot -Tpng > graph.png","title":"terraform graph"},{"location":"usage/#module-list","text":"What: List module inventory from embedded/custom roots. Flags: --modules/-m (modules root), --output/-o ( table|json|yaml ) Example: pltf module list -m ./modules -o json","title":"module list"},{"location":"usage/#module-get","text":"What: Show module details (inputs/outputs). Flags: Same as module list. Example: pltf module get aws_eks -m ./modules","title":"module get"},{"location":"usage/#module-init","text":"What: Generate module.yaml from an existing Terraform module dir. Flags: --path (module dir), --name , --type , --description , --out , --force (overwrite) Example: pltf module init --path ./modules/aws_eks --force","title":"module init"},{"location":"usage/#validate-lint","text":"Structural validation plus lint suggestions (labels, unused vars). pltf validate -f env.yaml -e prod pltf validate -f service.yaml -e dev","title":"Validate + Lint"},{"location":"usage/#generate_1","text":"Render Terraform without running it. File inputs that point to existing files (relative to the spec) are copied into the output directory and paths are updated. pltf generate -f env.yaml -e dev pltf generate -f service.yaml -e prod -o .pltf/service/prod pltf generate -f service.yaml -e dev -m ./custom-mods --var cluster_name=my-dev Flags: - --modules/-m custom modules root. Modules with source: custom are resolved only from the custom root; others fall back to embedded modules. - --out/-o output dir (defaults .pltf/<env_name>/env/<env> or .pltf/<env_name>/<service>/<env> ). - --var/-v merges vars (env vars \u2192 service envRef vars \u2192 CLI vars).","title":"Generate"},{"location":"usage/#terraform-helpers","text":"Terraform commands live under pltf terraform ... and auto-generate before running TF. pltf terraform plan -f service.yaml -e dev # plan (supports --target, --parallelism, --detailed-exitcode, --plan-file) pltf terraform apply -f env.yaml -e prod # apply pltf terraform destroy -f env.yaml -e prod # destroy pltf terraform output -f service.yaml -e dev # outputs (--json supported) pltf terraform force-unlock -f env.yaml -e prod --lock-id=<id> Common flags: --target/-t , --parallelism/-p , --lock/-l , --lock-timeout/-T , --no-color/-C , --input/-i , --refresh/-r , --plan-file/-P , --detailed-exitcode/-d , --json/-j .","title":"Terraform helpers"},{"location":"usage/#preview_1","text":"Quick summary (provider, backend, labels, modules) without TF. pltf preview -f env.yaml -e prod","title":"Preview"},{"location":"usage/#module-inventory","text":"pltf module list [-m ./modules] [-o table|json|yaml] pltf module get aws_eks [-m ./modules] [-o table|json|yaml] pltf module init --path ./modules/aws_eks [--force]","title":"Module inventory"},{"location":"usage/#custom-backends","text":"In env spec backend.type can be s3|gcs|azurerm regardless of provider. Optional region , container , resource_group , profile (S3) supported.","title":"Custom backends"},{"location":"usage/#custom-modules","text":"Mark a module with source: custom to force lookup in your custom modules root. Provide a custom root via --modules or profile modules_root ; embedded modules remain available for everything else. Generate module.yaml for your own TF module with pltf module init --path <module_dir> [--force] .","title":"Custom modules"},{"location":"usage/#environment-defaults","text":"PLTF_DEFAULT_ENV or profile default_env let you omit --env when only one environment applies.","title":"Environment defaults"},{"location":"usage/#completions","text":"pltf completion bash|zsh|fish|powershell","title":"Completions"},{"location":"concepts/environment/","text":"Environment The common frame that powers your infrastructure. What is an Environment? Environment specs declare which cloud/account/region to configure. From this file, pltf can create the base resources (e.g., Kubernetes clusters, networks, IAM roles, ingress). You\u2019ll usually have one per staging/prod/QA; you can also create per-engineer or per-PR environments for isolated sandboxes. Definition (YAML) apiVersion: platform.io/v1 kind: Environment metadata: name: example-aws org: myorg provider: aws # cloud provider labels: team: platform backend: type: s3 # s3|gcs|azurerm bucket: my-tf-bucket # optional; auto-named if omitted region: us-east-1 environments: prod: account: \"123456789012\" region: us-east-1 variables: base_domain: prod.example.com modules: - id: base type: aws_base - id: eks type: aws_eks inputs: cluster_name: var.base_domain Key points: - metadata sets name/org/provider and optional labels (become global tags). - backend.type can be s3|gcs|azurerm (independent of provider). backend.profile supports cross-account S3. - environments map holds per-env account/region/vars/secrets; pick one via --env or profile default_env . - modules are shared across services; use embedded catalog or source: custom to pull from your own root. State Storage pltf uses your cloud\u2019s native bucket for remote state (S3/GCS/Azurerm). One bucket per environment; state and metadata for the environment and its services live as separate objects. Backends are managed via backend.* and can be cross-cloud (e.g., Azure env with S3 backend). Next Steps Learn about Modules . Explore Service (coming soon) to connect workloads to environments.","title":"Environment"},{"location":"concepts/environment/#environment","text":"The common frame that powers your infrastructure.","title":"Environment"},{"location":"concepts/environment/#what-is-an-environment","text":"Environment specs declare which cloud/account/region to configure. From this file, pltf can create the base resources (e.g., Kubernetes clusters, networks, IAM roles, ingress). You\u2019ll usually have one per staging/prod/QA; you can also create per-engineer or per-PR environments for isolated sandboxes.","title":"What is an Environment?"},{"location":"concepts/environment/#definition-yaml","text":"apiVersion: platform.io/v1 kind: Environment metadata: name: example-aws org: myorg provider: aws # cloud provider labels: team: platform backend: type: s3 # s3|gcs|azurerm bucket: my-tf-bucket # optional; auto-named if omitted region: us-east-1 environments: prod: account: \"123456789012\" region: us-east-1 variables: base_domain: prod.example.com modules: - id: base type: aws_base - id: eks type: aws_eks inputs: cluster_name: var.base_domain Key points: - metadata sets name/org/provider and optional labels (become global tags). - backend.type can be s3|gcs|azurerm (independent of provider). backend.profile supports cross-account S3. - environments map holds per-env account/region/vars/secrets; pick one via --env or profile default_env . - modules are shared across services; use embedded catalog or source: custom to pull from your own root.","title":"Definition (YAML)"},{"location":"concepts/environment/#state-storage","text":"pltf uses your cloud\u2019s native bucket for remote state (S3/GCS/Azurerm). One bucket per environment; state and metadata for the environment and its services live as separate objects. Backends are managed via backend.* and can be cross-cloud (e.g., Azure env with S3 backend).","title":"State Storage"},{"location":"concepts/environment/#next-steps","text":"Learn about Modules . Explore Service (coming soon) to connect workloads to environments.","title":"Next Steps"},{"location":"concepts/layer/","text":"Layer (Service) An independently managed set of modules. What is a Layer? You can put all modules in an Environment file, but for finer granularity you define layers (Services). A layer provisions a set of modules together as a single unit and links to an Environment. Layers have: - a unique name - the environment(s) they run in (via metadata.ref / envRef ) - a list of modules (with optional links and inputs) When to use layers? Break down a large environment into separately maintained stacks. Share module definitions across multiple environments without duplicating YAML. Isolate per-service concerns (e.g., app plus its database) while reusing environment foundations. Definition (YAML) Example: a service (layer) for a Kubernetes workload with a database. apiVersion: platform.io/v1 kind: Service metadata: name: payments-api ref: ../env.yaml # link to Environment envRef: prod: {} # environment keys supported modules: - id: app type: aws_k8s_service inputs: public_uri: \"/payments\" image: \"ghcr.io/acme/payments:latest\" links: readwrite: - db - id: db type: aws_postgres inputs: instance_class: db.t3.medium Notes: - Service name maps to ${layer_name} placeholder; ${env_name} is the environment key. - Modules can link to each other ( links ) to consume outputs without manual wiring (e.g., ${module.db.db_host} ). - Per-environment overrides live under metadata.envRef . Next Steps See Environment for foundations. Explore module details in References .","title":"Layer"},{"location":"concepts/layer/#layer-service","text":"An independently managed set of modules.","title":"Layer (Service)"},{"location":"concepts/layer/#what-is-a-layer","text":"You can put all modules in an Environment file, but for finer granularity you define layers (Services). A layer provisions a set of modules together as a single unit and links to an Environment. Layers have: - a unique name - the environment(s) they run in (via metadata.ref / envRef ) - a list of modules (with optional links and inputs)","title":"What is a Layer?"},{"location":"concepts/layer/#when-to-use-layers","text":"Break down a large environment into separately maintained stacks. Share module definitions across multiple environments without duplicating YAML. Isolate per-service concerns (e.g., app plus its database) while reusing environment foundations.","title":"When to use layers?"},{"location":"concepts/layer/#definition-yaml","text":"Example: a service (layer) for a Kubernetes workload with a database. apiVersion: platform.io/v1 kind: Service metadata: name: payments-api ref: ../env.yaml # link to Environment envRef: prod: {} # environment keys supported modules: - id: app type: aws_k8s_service inputs: public_uri: \"/payments\" image: \"ghcr.io/acme/payments:latest\" links: readwrite: - db - id: db type: aws_postgres inputs: instance_class: db.t3.medium Notes: - Service name maps to ${layer_name} placeholder; ${env_name} is the environment key. - Modules can link to each other ( links ) to consume outputs without manual wiring (e.g., ${module.db.db_host} ). - Per-environment overrides live under metadata.envRef .","title":"Definition (YAML)"},{"location":"concepts/layer/#next-steps","text":"See Environment for foundations. Explore module details in References .","title":"Next Steps"},{"location":"concepts/module/","text":"Module A high-level building block to provision infrastructure. What is a Module? pltf includes an embedded library of modules you can connect to build your stack. Each module is a high-level construct that provisions the resources needed to achieve its goal (e.g., EKS cluster, S3 bucket, Postgres). Modules are described by module.yaml (type/provider/version/inputs/outputs) and referenced in your Environment or Service spec. Definition Modules have: - a type (e.g., aws_eks , aws_s3 ) - an optional id/name (so you can include multiple of the same type) - optional inputs (configuration) - optional links (to consume other module outputs) - optional source ( custom forces lookup in your custom modules root) Modules are defined inside the modules section of an Environment or Service. Minimal configuration We built pltf so you can provision a resource with a single line. Defaults follow best practices; customize only what you need. modules: - id: cluster type: aws_eks - id: db type: aws_postgres Extra configuration Override only the fields you care about; pltf uses recommended defaults otherwise. modules: - id: devcluster type: aws_eks inputs: node_instance_type: t3.medium max_nodes: 5 spot_instances: true - id: dbfrontend type: aws_postgres inputs: instance_class: db.t3.medium engine_version: \"12.4\" Links (module outputs as inputs) Modules can consume outputs from others using links or direct references like ${module.redis.cache_host} . modules: - id: redis type: aws_redis - id: airflow type: helm_chart inputs: repository: https://airflow.apache.org chart: airflow namespace: airflow chart_version: 1.4.0 values: brokerUrl: \"rediss://:${module.redis.cache_auth_token}@${module.redis.cache_host}\" Custom modules Generate module.yaml for your own Terraform module via pltf module init --path <module_dir> [--force] . Use source: custom in specs and provide --modules (or profile modules_root ) to load them. Terraform compatible pltf uses Terraform under the hood, so you\u2019re never locked in. Extend with your own Terraform or take the generated code with you. Next Steps Learn about Layer/Service (coming soon). Explore the module API in References and per-module pages.","title":"Module"},{"location":"concepts/module/#module","text":"A high-level building block to provision infrastructure.","title":"Module"},{"location":"concepts/module/#what-is-a-module","text":"pltf includes an embedded library of modules you can connect to build your stack. Each module is a high-level construct that provisions the resources needed to achieve its goal (e.g., EKS cluster, S3 bucket, Postgres). Modules are described by module.yaml (type/provider/version/inputs/outputs) and referenced in your Environment or Service spec.","title":"What is a Module?"},{"location":"concepts/module/#definition","text":"Modules have: - a type (e.g., aws_eks , aws_s3 ) - an optional id/name (so you can include multiple of the same type) - optional inputs (configuration) - optional links (to consume other module outputs) - optional source ( custom forces lookup in your custom modules root) Modules are defined inside the modules section of an Environment or Service.","title":"Definition"},{"location":"concepts/module/#minimal-configuration","text":"We built pltf so you can provision a resource with a single line. Defaults follow best practices; customize only what you need. modules: - id: cluster type: aws_eks - id: db type: aws_postgres","title":"Minimal configuration"},{"location":"concepts/module/#extra-configuration","text":"Override only the fields you care about; pltf uses recommended defaults otherwise. modules: - id: devcluster type: aws_eks inputs: node_instance_type: t3.medium max_nodes: 5 spot_instances: true - id: dbfrontend type: aws_postgres inputs: instance_class: db.t3.medium engine_version: \"12.4\"","title":"Extra configuration"},{"location":"concepts/module/#links-module-outputs-as-inputs","text":"Modules can consume outputs from others using links or direct references like ${module.redis.cache_host} . modules: - id: redis type: aws_redis - id: airflow type: helm_chart inputs: repository: https://airflow.apache.org chart: airflow namespace: airflow chart_version: 1.4.0 values: brokerUrl: \"rediss://:${module.redis.cache_auth_token}@${module.redis.cache_host}\"","title":"Links (module outputs as inputs)"},{"location":"concepts/module/#custom-modules","text":"Generate module.yaml for your own Terraform module via pltf module init --path <module_dir> [--force] . Use source: custom in specs and provide --modules (or profile modules_root ) to load them.","title":"Custom modules"},{"location":"concepts/module/#terraform-compatible","text":"pltf uses Terraform under the hood, so you\u2019re never locked in. Extend with your own Terraform or take the generated code with you.","title":"Terraform compatible"},{"location":"concepts/module/#next-steps","text":"Learn about Layer/Service (coming soon). Explore the module API in References and per-module pages.","title":"Next Steps"},{"location":"concepts/overview/","text":"Concepts Overview pltf is Infrastructure-as-Code with a higher-level abstraction. You write configuration files, then run the pltf CLI (locally or in CI/CD) to connect to your cloud account and provision resources using Terraform under the hood. How It Works 1) Author YAML specs. 2) Run pltf preview|validate|generate|terraform ... . 3) The CLI renders Terraform (providers, backends, locals, remote state) and can execute Terraform for you. There are two primary spec types: Environment : Specifies cloud, account, and region. Running an Environment sets up the base resources (e.g., Kubernetes cluster, networks, IAM, ingress). Typical patterns: one per staging/prod/QA, or one per engineer/PR for isolated sandboxes. Service (Layer): Specifies the workload (often a microservice) and any non-Kubernetes resources it needs (e.g., databases, queues). pltf connects these seamlessly to the Environment. Environment and Service specs link via metadata.ref (path to env) and envRef (per-environment overrides).","title":"Overview"},{"location":"concepts/overview/#concepts-overview","text":"pltf is Infrastructure-as-Code with a higher-level abstraction. You write configuration files, then run the pltf CLI (locally or in CI/CD) to connect to your cloud account and provision resources using Terraform under the hood.","title":"Concepts Overview"},{"location":"concepts/overview/#how-it-works","text":"1) Author YAML specs. 2) Run pltf preview|validate|generate|terraform ... . 3) The CLI renders Terraform (providers, backends, locals, remote state) and can execute Terraform for you. There are two primary spec types: Environment : Specifies cloud, account, and region. Running an Environment sets up the base resources (e.g., Kubernetes cluster, networks, IAM, ingress). Typical patterns: one per staging/prod/QA, or one per engineer/PR for isolated sandboxes. Service (Layer): Specifies the workload (often a microservice) and any non-Kubernetes resources it needs (e.g., databases, queues). pltf connects these seamlessly to the Environment. Environment and Service specs link via metadata.ref (path to env) and envRef (per-environment overrides).","title":"How It Works"},{"location":"features/backends/","text":"Backends Choose where Terraform state lives, independent of the target cloud. What it does Supports backend.type = s3|gcs|azurerm for any provider. Allows backend.profile for cross-account S3, region override, and container/resource_group for azurerm. Ensures the backend bucket/container exists before running Terraform. Example backend: type: s3 bucket: platform-tfstate region: us-east-1 profile: ops-account Notes Backends are rendered into backend.tf / terraform.tfvars alongside providers. You can point all clouds to a single backend (e.g., S3) if desired.","title":"Backends"},{"location":"features/backends/#backends","text":"Choose where Terraform state lives, independent of the target cloud.","title":"Backends"},{"location":"features/backends/#what-it-does","text":"Supports backend.type = s3|gcs|azurerm for any provider. Allows backend.profile for cross-account S3, region override, and container/resource_group for azurerm. Ensures the backend bucket/container exists before running Terraform.","title":"What it does"},{"location":"features/backends/#example","text":"backend: type: s3 bucket: platform-tfstate region: us-east-1 profile: ops-account","title":"Example"},{"location":"features/backends/#notes","text":"Backends are rendered into backend.tf / terraform.tfvars alongside providers. You can point all clouds to a single backend (e.g., S3) if desired.","title":"Notes"},{"location":"features/custom-modules/","text":"Custom Modules Mix embedded modules with your own Terraform modules. What it does Uses the embedded catalog by default. Supports a custom modules root ( --modules or profile modules_root ). source: custom on a module forces lookup in your custom root; others fall back to embedded. pltf module init inspects a TF module and writes module.yaml metadata. Inventory commands: pltf module list|get -o table|json|yaml . Example modules: - name: app type: my_custom_service source: custom image: ghcr.io/acme/app:latest Notes Custom and embedded modules can coexist in the same spec. Module metadata ( module.yaml ) drives inputs/outputs and wiring; keep it committed.","title":"Custom Modules"},{"location":"features/custom-modules/#custom-modules","text":"Mix embedded modules with your own Terraform modules.","title":"Custom Modules"},{"location":"features/custom-modules/#what-it-does","text":"Uses the embedded catalog by default. Supports a custom modules root ( --modules or profile modules_root ). source: custom on a module forces lookup in your custom root; others fall back to embedded. pltf module init inspects a TF module and writes module.yaml metadata. Inventory commands: pltf module list|get -o table|json|yaml .","title":"What it does"},{"location":"features/custom-modules/#example","text":"modules: - name: app type: my_custom_service source: custom image: ghcr.io/acme/app:latest","title":"Example"},{"location":"features/custom-modules/#notes","text":"Custom and embedded modules can coexist in the same spec. Module metadata ( module.yaml ) drives inputs/outputs and wiring; keep it committed.","title":"Notes"},{"location":"features/placeholders/","text":"Placeholders & Wiring Lightweight templating to keep specs DRY and wire modules together. What it does Intrinsics: ${env_name} , ${layer_name} . References: ${module.<id>.<output>} , ${parent.<output>} (services), ${var.<name>} . Auto-wires inputs to outputs when names match within scope; missing required values fail validation. Examples public_uri: \"https://${module.dns.domain}\" bucket_name: \"app-${env_name}\" max_nodes: \"${var.max_nodes}\" public_url: \"${parent.domain}/hello\" # in a service spec Notes Services can reference parent env outputs via ${parent.*} . Variables precedence: env vars \u2192 service envRef vars \u2192 CLI --var .","title":"Placeholders & Wiring"},{"location":"features/placeholders/#placeholders-wiring","text":"Lightweight templating to keep specs DRY and wire modules together.","title":"Placeholders &amp; Wiring"},{"location":"features/placeholders/#what-it-does","text":"Intrinsics: ${env_name} , ${layer_name} . References: ${module.<id>.<output>} , ${parent.<output>} (services), ${var.<name>} . Auto-wires inputs to outputs when names match within scope; missing required values fail validation.","title":"What it does"},{"location":"features/placeholders/#examples","text":"public_uri: \"https://${module.dns.domain}\" bucket_name: \"app-${env_name}\" max_nodes: \"${var.max_nodes}\" public_url: \"${parent.domain}/hello\" # in a service spec","title":"Examples"},{"location":"features/placeholders/#notes","text":"Services can reference parent env outputs via ${parent.*} . Variables precedence: env vars \u2192 service envRef vars \u2192 CLI --var .","title":"Notes"},{"location":"features/preview/","text":"Preview Quickly inspect what will be generated without touching Terraform. What it does pltf preview reads your spec and shows provider, backend type, labels, and modules that will render. Auto-detects env vs service based on kind . Example pltf preview -f env.yaml -e prod Notes No cloud credentials needed; useful in CI or pre-commit checks. Pair with pltf validate for faster feedback before generation/apply.","title":"Preview"},{"location":"features/preview/#preview","text":"Quickly inspect what will be generated without touching Terraform.","title":"Preview"},{"location":"features/preview/#what-it-does","text":"pltf preview reads your spec and shows provider, backend type, labels, and modules that will render. Auto-detects env vs service based on kind .","title":"What it does"},{"location":"features/preview/#example","text":"pltf preview -f env.yaml -e prod","title":"Example"},{"location":"features/preview/#notes","text":"No cloud credentials needed; useful in CI or pre-commit checks. Pair with pltf validate for faster feedback before generation/apply.","title":"Notes"},{"location":"features/profiles/","text":"Profiles & Defaults Set org-wide defaults so users type fewer flags and stay consistent. What it does Reads ~/.pltf/profile.yaml (or PLTF_PROFILE ) for defaults like modules_root , default_env , default_out , and telemetry . Lets you pick a custom modules root for all commands without repeating --modules . Allows a default environment name so --env can be omitted when unambiguous. Example profile modules_root: /infra/modules default_env: dev default_out: .pltf telemetry: false Usage Any CLI flags you pass override profile settings. Profiles are optional; when absent, embedded modules and CLI flags are used.","title":"Profiles & Defaults"},{"location":"features/profiles/#profiles-defaults","text":"Set org-wide defaults so users type fewer flags and stay consistent.","title":"Profiles &amp; Defaults"},{"location":"features/profiles/#what-it-does","text":"Reads ~/.pltf/profile.yaml (or PLTF_PROFILE ) for defaults like modules_root , default_env , default_out , and telemetry . Lets you pick a custom modules root for all commands without repeating --modules . Allows a default environment name so --env can be omitted when unambiguous.","title":"What it does"},{"location":"features/profiles/#example-profile","text":"modules_root: /infra/modules default_env: dev default_out: .pltf telemetry: false","title":"Example profile"},{"location":"features/profiles/#usage","text":"Any CLI flags you pass override profile settings. Profiles are optional; when absent, embedded modules and CLI flags are used.","title":"Usage"},{"location":"features/secrets/","text":"Secrets Keep sensitive values out of specs and source control. What it does Secrets stay as Terraform variables and render to Kubernetes secrets for services. You declare secret keys in your spec; actual values are provided at runtime via environment variables or --var , typically sourced from your secret store/CI. Services receive secrets as env vars; no values are written into locals or files. Example (service) spec: secrets: db_password: {} # value supplied via env/CI modules: - type: aws_k8s_service name: app env_vars: - name: DB_PASSWORD value: \"${var.db_password}\" Runtime: PLTF_VAR_db_password=supersecret pltf terraform apply -f service.yaml -e prod Notes Prefer env/CI secret stores; do not commit secret values to specs or repos. Services restart to pick up new secret values after apply; plan rotations accordingly.","title":"Secrets"},{"location":"features/secrets/#secrets","text":"Keep sensitive values out of specs and source control.","title":"Secrets"},{"location":"features/secrets/#what-it-does","text":"Secrets stay as Terraform variables and render to Kubernetes secrets for services. You declare secret keys in your spec; actual values are provided at runtime via environment variables or --var , typically sourced from your secret store/CI. Services receive secrets as env vars; no values are written into locals or files.","title":"What it does"},{"location":"features/secrets/#example-service","text":"spec: secrets: db_password: {} # value supplied via env/CI modules: - type: aws_k8s_service name: app env_vars: - name: DB_PASSWORD value: \"${var.db_password}\" Runtime: PLTF_VAR_db_password=supersecret pltf terraform apply -f service.yaml -e prod","title":"Example (service)"},{"location":"features/secrets/#notes","text":"Prefer env/CI secret stores; do not commit secret values to specs or repos. Services restart to pick up new secret values after apply; plan rotations accordingly.","title":"Notes"},{"location":"features/telemetry/","text":"Telemetry Optional usage reporting. What it does Uses a global --telemetry flag (defaults from profile) to enable/disable future analytics. Currently a stub/no-op; reserved for opt-in reporting. Usage Set in profile: yaml telemetry: false Or export PLTF_TELEMETRY=0 to disable.","title":"Telemetry"},{"location":"features/telemetry/#telemetry","text":"Optional usage reporting.","title":"Telemetry"},{"location":"features/telemetry/#what-it-does","text":"Uses a global --telemetry flag (defaults from profile) to enable/disable future analytics. Currently a stub/no-op; reserved for opt-in reporting.","title":"What it does"},{"location":"features/telemetry/#usage","text":"Set in profile: yaml telemetry: false Or export PLTF_TELEMETRY=0 to disable.","title":"Usage"},{"location":"features/terraform-commands/","text":"Terraform Commands Run Terraform with consistent, auto-generated configs. What it does Commands live under pltf terraform plan|apply|destroy|output|force-unlock . Auto-generates Terraform (providers, backends, modules, outputs) before running TF. Ensures the backend bucket/container exists (S3/GCS/Azurerm) before init/apply. Passes through standard TF flags (targets, parallelism, lock, no-color, plan file, detailed exit codes). Examples pltf terraform plan -f service.yaml -e dev --detailed-exitcode --plan-file=/tmp/plan.tfplan pltf terraform apply -f env.yaml -e prod pltf terraform destroy -f env.yaml -e prod pltf terraform output -f service.yaml -e dev --json pltf terraform force-unlock -f env.yaml -e prod --lock-id=12345 Notes Backends are decoupled from provider ( s3|gcs|azurerm supported). Common flags: --target/-t , --parallelism/-p , --lock/-l , --lock-timeout/-T , --no-color/-C , --input/-i , --refresh/-r , --plan-file/-P , --detailed-exitcode/-d , --json/-j . Uses the same generation path as pltf generate ; you can inspect the rendered TF in the output directory.","title":"Terraform Commands"},{"location":"features/terraform-commands/#terraform-commands","text":"Run Terraform with consistent, auto-generated configs.","title":"Terraform Commands"},{"location":"features/terraform-commands/#what-it-does","text":"Commands live under pltf terraform plan|apply|destroy|output|force-unlock . Auto-generates Terraform (providers, backends, modules, outputs) before running TF. Ensures the backend bucket/container exists (S3/GCS/Azurerm) before init/apply. Passes through standard TF flags (targets, parallelism, lock, no-color, plan file, detailed exit codes).","title":"What it does"},{"location":"features/terraform-commands/#examples","text":"pltf terraform plan -f service.yaml -e dev --detailed-exitcode --plan-file=/tmp/plan.tfplan pltf terraform apply -f env.yaml -e prod pltf terraform destroy -f env.yaml -e prod pltf terraform output -f service.yaml -e dev --json pltf terraform force-unlock -f env.yaml -e prod --lock-id=12345","title":"Examples"},{"location":"features/terraform-commands/#notes","text":"Backends are decoupled from provider ( s3|gcs|azurerm supported). Common flags: --target/-t , --parallelism/-p , --lock/-l , --lock-timeout/-T , --no-color/-C , --input/-i , --refresh/-r , --plan-file/-P , --detailed-exitcode/-d , --json/-j . Uses the same generation path as pltf generate ; you can inspect the rendered TF in the output directory.","title":"Notes"},{"location":"features/terraform-generator/","text":"Terraform Generator Render Terraform from your specs without applying. Ideal for reviews, CI, or migrating to raw Terraform. Overview pltf generate reads Environment or Service specs, auto-detects kind, and writes a self-contained Terraform directory (providers, backend, modules, outputs, versions). No cloud credentials are required to render. Generate Terraform Environment: pltf generate -f env.yaml -e prod -o .pltf/env/prod # produces: # .pltf/env/prod/ # \u251c\u2500 modules/ # copied/embedded module code used by this stack # \u251c\u2500 providers.tf # provider blocks + required versions # \u251c\u2500 backend.tf # state backend (s3|gcs|azurerm) # \u251c\u2500 locals.tf # computed locals/labels # \u251c\u2500 modules-*.tf # module instantiations # \u251c\u2500 outputs.tf # outputs # \u2514\u2500 versions.tf # provider/Terraform constraints Service: pltf generate -f service.yaml -e prod -o .pltf/service/payments/prod Migrate to Terraform Run pltf generate (or pltf terraform plan to generate+init) for each env/service stack. Commit the generated directory to VCS if you want to manage TF directly. Backends follow your spec; use backend.type ( s3|gcs|azurerm ) to point at your state bucket/container. Notes No provider calls during generation; safe to run without credentials. Supports custom modules ( source: custom ) alongside embedded ones; the generated modules/ directory is self-contained. Use pltf preview first to sanity check provider/backend/modules before generation.","title":"Terraform Generator"},{"location":"features/terraform-generator/#terraform-generator","text":"Render Terraform from your specs without applying. Ideal for reviews, CI, or migrating to raw Terraform.","title":"Terraform Generator"},{"location":"features/terraform-generator/#overview","text":"pltf generate reads Environment or Service specs, auto-detects kind, and writes a self-contained Terraform directory (providers, backend, modules, outputs, versions). No cloud credentials are required to render.","title":"Overview"},{"location":"features/terraform-generator/#generate-terraform","text":"Environment: pltf generate -f env.yaml -e prod -o .pltf/env/prod # produces: # .pltf/env/prod/ # \u251c\u2500 modules/ # copied/embedded module code used by this stack # \u251c\u2500 providers.tf # provider blocks + required versions # \u251c\u2500 backend.tf # state backend (s3|gcs|azurerm) # \u251c\u2500 locals.tf # computed locals/labels # \u251c\u2500 modules-*.tf # module instantiations # \u251c\u2500 outputs.tf # outputs # \u2514\u2500 versions.tf # provider/Terraform constraints Service: pltf generate -f service.yaml -e prod -o .pltf/service/payments/prod","title":"Generate Terraform"},{"location":"features/terraform-generator/#migrate-to-terraform","text":"Run pltf generate (or pltf terraform plan to generate+init) for each env/service stack. Commit the generated directory to VCS if you want to manage TF directly. Backends follow your spec; use backend.type ( s3|gcs|azurerm ) to point at your state bucket/container.","title":"Migrate to Terraform"},{"location":"features/terraform-generator/#notes","text":"No provider calls during generation; safe to run without credentials. Supports custom modules ( source: custom ) alongside embedded ones; the generated modules/ directory is self-contained. Use pltf preview first to sanity check provider/backend/modules before generation.","title":"Notes"},{"location":"features/validation/","text":"Validation & Lint Catch spec issues early before generation or apply. What it does pltf validate runs structural validation for Environment and Service specs. Built-in lint suggests labels and flags unused variables. Auto-detects kind (env/service) and applies the right checks. Example pltf validate -f env.yaml -e prod pltf validate -f service.yaml -e dev Notes Lint also runs implicitly during validate. Combine with pltf preview to sanity check providers/backends/modules.","title":"Validation & Lint"},{"location":"features/validation/#validation-lint","text":"Catch spec issues early before generation or apply.","title":"Validation &amp; Lint"},{"location":"features/validation/#what-it-does","text":"pltf validate runs structural validation for Environment and Service specs. Built-in lint suggests labels and flags unused variables. Auto-detects kind (env/service) and applies the right checks.","title":"What it does"},{"location":"features/validation/#example","text":"pltf validate -f env.yaml -e prod pltf validate -f service.yaml -e dev","title":"Example"},{"location":"features/validation/#notes","text":"Lint also runs implicitly during validate. Combine with pltf preview to sanity check providers/backends/modules.","title":"Notes"},{"location":"features/variables/","text":"Variables Reuse specs across environments and services with minimal templating. Overview Variables can be declared in your specs and overridden at runtime. They resolve into Terraform variables so you can keep a single spec and tune it per environment. Declare variables Define inputs in your spec and reference them with ${var.<name>} : variables: min_nodes: \"2\" max_nodes: \"5\" modules: - type: aws_eks min_nodes: \"${var.min_nodes}\" max_nodes: \"${var.max_nodes}\" Override at runtime Use repeatable --var flags or environment variables: pltf terraform apply -f env.yaml -e prod --var min_nodes=3 --var max_nodes=6 # or PLTF_VAR_min_nodes=3 PLTF_VAR_max_nodes=6 pltf generate -f env.yaml -e prod Environment-scoped variables Service specs can declare per-environment variables under envRef : envRef: name: prod path: ./env.yaml variables: containers: 5 modules: - type: aws_k8s_service min_containers: 1 max_containers: \"${var.containers}\" Parent outputs Services can use environment outputs via ${parent.<output>} : public_uri: \"${parent.domain}/hello\" Placeholder catalog ${env_name} , ${layer_name} (intrinsics) ${module.<module_name>.<output_name>} ${parent.<output_name>} (service only) ${var.<name>} (declared variables or CLI/env overrides) Notes Required variables without defaults must be provided via --var or env. Precedence: env vars \u2192 service envRef vars \u2192 CLI --var . Values stay in Terraform variables (not locals) to avoid leaking secrets.","title":"Variables"},{"location":"features/variables/#variables","text":"Reuse specs across environments and services with minimal templating.","title":"Variables"},{"location":"features/variables/#overview","text":"Variables can be declared in your specs and overridden at runtime. They resolve into Terraform variables so you can keep a single spec and tune it per environment.","title":"Overview"},{"location":"features/variables/#declare-variables","text":"Define inputs in your spec and reference them with ${var.<name>} : variables: min_nodes: \"2\" max_nodes: \"5\" modules: - type: aws_eks min_nodes: \"${var.min_nodes}\" max_nodes: \"${var.max_nodes}\"","title":"Declare variables"},{"location":"features/variables/#override-at-runtime","text":"Use repeatable --var flags or environment variables: pltf terraform apply -f env.yaml -e prod --var min_nodes=3 --var max_nodes=6 # or PLTF_VAR_min_nodes=3 PLTF_VAR_max_nodes=6 pltf generate -f env.yaml -e prod","title":"Override at runtime"},{"location":"features/variables/#environment-scoped-variables","text":"Service specs can declare per-environment variables under envRef : envRef: name: prod path: ./env.yaml variables: containers: 5 modules: - type: aws_k8s_service min_containers: 1 max_containers: \"${var.containers}\"","title":"Environment-scoped variables"},{"location":"features/variables/#parent-outputs","text":"Services can use environment outputs via ${parent.<output>} : public_uri: \"${parent.domain}/hello\"","title":"Parent outputs"},{"location":"features/variables/#placeholder-catalog","text":"${env_name} , ${layer_name} (intrinsics) ${module.<module_name>.<output_name>} ${parent.<output_name>} (service only) ${var.<name>} (declared variables or CLI/env overrides)","title":"Placeholder catalog"},{"location":"features/variables/#notes","text":"Required variables without defaults must be provided via --var or env. Precedence: env vars \u2192 service envRef vars \u2192 CLI --var . Values stay in Terraform variables (not locals) to avoid leaking secrets.","title":"Notes"},{"location":"getting-started/aws/","text":"Getting Started: AWS This guide walks through provisioning a simple environment and service on AWS using pltf. You will create two YAML specs (environment and service), generate Terraform, and deploy. 1) Prerequisites Terraform v1.5+ (installed locally or via your CI) Docker (to build/push your images if needed) AWS credentials configured in your shell (e.g., aws configure or environment variables) (Optional) Custom modules directory if you want to bring your own modules Install pltf (Homebrew): brew tap yindia/pltf brew install pltf Or use the install script: /bin/bash -c \\\"$(curl -fsSL https://raw.githubusercontent.com/your-org/pltf/main/install.sh)\\\" 2) Create an Environment spec Create env.yaml : apiVersion: platform.io/v1 kind: Environment metadata: name: example-aws org: demo provider: aws labels: team: platform cost_center: shared environments: prod: account: \\\"123456789012\\\" region: us-east-1 backend: type: s3 # state backend (s3|gcs|azurerm) profile: default # optional cross-account profile variables: base_domain: prod.demo.internal cluster_name: demo-eks modules: - type: aws_base - type: aws_eks - type: aws_k8s_base What this does: - Configures AWS provider/region and S3 backend. - Creates networking, an EKS cluster, and base Kubernetes add-ons. - Exposes outputs (e.g., cluster endpoint/CA) for services. Generate and apply: pltf terraform plan -f env.yaml -e prod pltf terraform apply -f env.yaml -e prod First apply can take ~15 minutes. You can inspect outputs: pltf terraform output -f env.yaml -e prod --json 3) Create a Service spec Create service.yaml : apiVersion: platform.io/v1 kind: Service metadata: name: payments-api org: demo provider: aws envRef: name: prod path: ./env.yaml spec: variables: image: ghcr.io/demo/payments:latest modules: - name: app type: aws_k8s_service port: http: 8080 public_uri: \\\"/payments\\\" links: - app-bucket: [write] - name: app-bucket type: aws_s3 bucket_name: \\\"payments-${env_name}\\\" What this does: - Deploys a Kubernetes service on the EKS cluster created above. - Provisions an S3 bucket and links it to the app with write permissions (IRSA policy is generated). Generate and apply: pltf terraform plan -f service.yaml -e prod pltf terraform apply -f service.yaml -e prod 4) Access the service Find the load balancer host from outputs: pltf terraform output -f service.yaml -e prod | grep load_balancer_raw_dns Curl the path: curl http://<lb>/payments 5) Cleanup pltf terraform destroy -f service.yaml -e prod pltf terraform destroy -f env.yaml -e prod 6) Next Steps Review AWS architecture and module docs in the References section. Add more modules (RDS, Redis, SES, SNS, SQS) and link them for IAM/IRSA wiring. Use profiles ( ~/.pltf/profile.yaml ) to set default env/modules root and cross-account backends.","title":"AWS"},{"location":"getting-started/aws/#getting-started-aws","text":"This guide walks through provisioning a simple environment and service on AWS using pltf. You will create two YAML specs (environment and service), generate Terraform, and deploy.","title":"Getting Started: AWS"},{"location":"getting-started/aws/#1-prerequisites","text":"Terraform v1.5+ (installed locally or via your CI) Docker (to build/push your images if needed) AWS credentials configured in your shell (e.g., aws configure or environment variables) (Optional) Custom modules directory if you want to bring your own modules Install pltf (Homebrew): brew tap yindia/pltf brew install pltf Or use the install script: /bin/bash -c \\\"$(curl -fsSL https://raw.githubusercontent.com/your-org/pltf/main/install.sh)\\\"","title":"1) Prerequisites"},{"location":"getting-started/aws/#2-create-an-environment-spec","text":"Create env.yaml : apiVersion: platform.io/v1 kind: Environment metadata: name: example-aws org: demo provider: aws labels: team: platform cost_center: shared environments: prod: account: \\\"123456789012\\\" region: us-east-1 backend: type: s3 # state backend (s3|gcs|azurerm) profile: default # optional cross-account profile variables: base_domain: prod.demo.internal cluster_name: demo-eks modules: - type: aws_base - type: aws_eks - type: aws_k8s_base What this does: - Configures AWS provider/region and S3 backend. - Creates networking, an EKS cluster, and base Kubernetes add-ons. - Exposes outputs (e.g., cluster endpoint/CA) for services. Generate and apply: pltf terraform plan -f env.yaml -e prod pltf terraform apply -f env.yaml -e prod First apply can take ~15 minutes. You can inspect outputs: pltf terraform output -f env.yaml -e prod --json","title":"2) Create an Environment spec"},{"location":"getting-started/aws/#3-create-a-service-spec","text":"Create service.yaml : apiVersion: platform.io/v1 kind: Service metadata: name: payments-api org: demo provider: aws envRef: name: prod path: ./env.yaml spec: variables: image: ghcr.io/demo/payments:latest modules: - name: app type: aws_k8s_service port: http: 8080 public_uri: \\\"/payments\\\" links: - app-bucket: [write] - name: app-bucket type: aws_s3 bucket_name: \\\"payments-${env_name}\\\" What this does: - Deploys a Kubernetes service on the EKS cluster created above. - Provisions an S3 bucket and links it to the app with write permissions (IRSA policy is generated). Generate and apply: pltf terraform plan -f service.yaml -e prod pltf terraform apply -f service.yaml -e prod","title":"3) Create a Service spec"},{"location":"getting-started/aws/#4-access-the-service","text":"Find the load balancer host from outputs: pltf terraform output -f service.yaml -e prod | grep load_balancer_raw_dns Curl the path: curl http://<lb>/payments","title":"4) Access the service"},{"location":"getting-started/aws/#5-cleanup","text":"pltf terraform destroy -f service.yaml -e prod pltf terraform destroy -f env.yaml -e prod","title":"5) Cleanup"},{"location":"getting-started/aws/#6-next-steps","text":"Review AWS architecture and module docs in the References section. Add more modules (RDS, Redis, SES, SNS, SQS) and link them for IAM/IRSA wiring. Use profiles ( ~/.pltf/profile.yaml ) to set default env/modules root and cross-account backends.","title":"6) Next Steps"},{"location":"getting-started/azure/","text":"Getting Started: Azure Placeholder. Provide steps for Azure environment/service, backend (azurerm or s3/gcs), and custom modules.","title":"Getting Started: Azure"},{"location":"getting-started/azure/#getting-started-azure","text":"Placeholder. Provide steps for Azure environment/service, backend (azurerm or s3/gcs), and custom modules.","title":"Getting Started: Azure"},{"location":"getting-started/gcp/","text":"Getting Started: GCP Placeholder. Provide steps for GCP environment/service, backend (gcs or s3), and custom modules.","title":"Getting Started: GCP"},{"location":"getting-started/gcp/#getting-started-gcp","text":"Placeholder. Provide steps for GCP environment/service, backend (gcs or s3), and custom modules.","title":"Getting Started: GCP"},{"location":"references/aws/","text":"AWS Reference AWS is fully supported for environments, services, and modules. This page summarizes how the AWS provider, backends, and module wiring work in pltf. Provider and Backends Provider: Automatically injected; version comes from the central versions file. Region is taken from your env spec. Backends: You can store state in s3 , gcs , or azurerm even when targeting AWS. For cross-account S3, set backend.profile . Optional backend.region overrides the bucket region. Default tags: Labels in your env/service specs become global tags on the AWS provider. Example (Environment + Service) apiVersion: platform.io/v1 kind: Environment metadata: name: example-aws org: pltf provider: aws labels: team: platform cost_center: shared environments: prod: account: \"556169302489\" region: us-east-1 backend: type: s3 profile: cross-account modules: - type: aws_base - type: aws_eks - type: aws_k8s_base apiVersion: platform.io/v1 kind: Service metadata: name: payments-api org: pltf provider: aws envRef: name: prod path: ../env.yaml spec: variables: image: ghcr.io/acme/payments:latest modules: - name: app type: aws_k8s_service port: http: 8080 links: - app-bucket: [write] - app-queue: [consume] - name: app-bucket type: aws_s3 bucket_name: \"payments-${env_name}\" - name: app-queue type: aws_sqs Modules and Fields Fields: Each module instance accepts inputs declared in its module.yaml . Only set what you need; defaults apply otherwise. Names: name is optional; defaults to the module type . Names are used for Terraform resource names and template placeholders. Types: type selects the module implementation. Embedded AWS modules are documented under \u201cModules (AWS)\u201d in the nav. Sources: Add source: custom to pull a module from your custom modules root; otherwise the embedded catalog is used. Linking Linking lets a module consume outputs of another: links: - app-bucket: [read, write] - app-queue: [consume] When links are present, pltf automatically renders IAM policies and (for Kubernetes) IRSA trusts. Supported AWS link targets include S3, SQS, SNS, SES, DynamoDB, RDS, and more via module metadata. Template placeholders ${env_name} and ${layer_name} become the resolved environment/service names. ${module.<module_name>.<output_name>} references another module\u2019s output. ${parent.<output_name>} references outputs from the parent environment when authoring a service. ${var.<name>} references variables defined in the spec or via --var . Useful commands pltf module list -o table \u2014 see available AWS modules. pltf module get aws_eks \u2014 inspect inputs/outputs. pltf generate -f env.yaml -e prod \u2014 render Terraform for AWS. pltf terraform plan/apply ... \u2014 generate + execute Terraform (plan/apply/destroy/output/force-unlock). See the module-specific pages under \u201cModules (AWS)\u201d for detailed inputs, outputs, and examples.","title":"Overview"},{"location":"references/aws/#aws-reference","text":"AWS is fully supported for environments, services, and modules. This page summarizes how the AWS provider, backends, and module wiring work in pltf.","title":"AWS Reference"},{"location":"references/aws/#provider-and-backends","text":"Provider: Automatically injected; version comes from the central versions file. Region is taken from your env spec. Backends: You can store state in s3 , gcs , or azurerm even when targeting AWS. For cross-account S3, set backend.profile . Optional backend.region overrides the bucket region. Default tags: Labels in your env/service specs become global tags on the AWS provider.","title":"Provider and Backends"},{"location":"references/aws/#example-environment-service","text":"apiVersion: platform.io/v1 kind: Environment metadata: name: example-aws org: pltf provider: aws labels: team: platform cost_center: shared environments: prod: account: \"556169302489\" region: us-east-1 backend: type: s3 profile: cross-account modules: - type: aws_base - type: aws_eks - type: aws_k8s_base apiVersion: platform.io/v1 kind: Service metadata: name: payments-api org: pltf provider: aws envRef: name: prod path: ../env.yaml spec: variables: image: ghcr.io/acme/payments:latest modules: - name: app type: aws_k8s_service port: http: 8080 links: - app-bucket: [write] - app-queue: [consume] - name: app-bucket type: aws_s3 bucket_name: \"payments-${env_name}\" - name: app-queue type: aws_sqs","title":"Example (Environment + Service)"},{"location":"references/aws/#modules-and-fields","text":"Fields: Each module instance accepts inputs declared in its module.yaml . Only set what you need; defaults apply otherwise. Names: name is optional; defaults to the module type . Names are used for Terraform resource names and template placeholders. Types: type selects the module implementation. Embedded AWS modules are documented under \u201cModules (AWS)\u201d in the nav. Sources: Add source: custom to pull a module from your custom modules root; otherwise the embedded catalog is used.","title":"Modules and Fields"},{"location":"references/aws/#linking","text":"Linking lets a module consume outputs of another: links: - app-bucket: [read, write] - app-queue: [consume] When links are present, pltf automatically renders IAM policies and (for Kubernetes) IRSA trusts. Supported AWS link targets include S3, SQS, SNS, SES, DynamoDB, RDS, and more via module metadata.","title":"Linking"},{"location":"references/aws/#template-placeholders","text":"${env_name} and ${layer_name} become the resolved environment/service names. ${module.<module_name>.<output_name>} references another module\u2019s output. ${parent.<output_name>} references outputs from the parent environment when authoring a service. ${var.<name>} references variables defined in the spec or via --var .","title":"Template placeholders"},{"location":"references/aws/#useful-commands","text":"pltf module list -o table \u2014 see available AWS modules. pltf module get aws_eks \u2014 inspect inputs/outputs. pltf generate -f env.yaml -e prod \u2014 render Terraform for AWS. pltf terraform plan/apply ... \u2014 generate + execute Terraform (plan/apply/destroy/output/force-unlock). See the module-specific pages under \u201cModules (AWS)\u201d for detailed inputs, outputs, and examples.","title":"Useful commands"},{"location":"references/aws_eks_access/","text":"AWS EKS Access How to access EKS clusters generated by pltf. Kubeconfig Fetch outputs: bash pltf terraform output -f env.yaml -e <env> --json | jq '.aws_eks' Note k8s_cluster_name , k8s_endpoint , and k8s_ca_data . Update kubeconfig: bash aws eks update-kubeconfig \\ --region <region> \\ --name <cluster> \\ --profile <aws-profile-if-needed> Use the same AWS profile that has access to the environment account (or backend profile if you share credentials). Generated Terraform already configures Kubernetes and Helm providers using these outputs when you run pltf terraform plan/apply . AWS IAM to Kubernetes RBAC EKS uses the aws-auth ConfigMap in kube-system to map IAM users/roles to Kubernetes groups. Example aws-auth data: apiVersion: v1 data: mapRoles: | - groups: ['system:bootstrappers', 'system:nodes'] rolearn: arn:aws:iam::ACCOUNT_ID:role/pltf-live-example-dev-eks-default-node-group username: system:node:{{EC2PrivateDNSName}} - groups: ['system:masters'] rolearn: arn:aws:iam::ACCOUNT_ID:role/demo-admin username: pltf-managed mapUsers: | - groups: ['system:masters'] userarn: arn:aws:iam::ACCOUNT_ID:user/demo-admin username: pltf-managed Fields: - rolearn / userarn : IAM principal. - username : friendly alias. - groups : Kubernetes RBAC groups (use system:masters for admin). Granting access via pltf Use admin_arns on aws_k8s_base to inject IAM admins without editing Kubernetes directly: modules: - type: aws_k8s_base admin_arns: - \"arn:aws:iam::123456789012:user/platform-admin\" - \"arn:aws:iam::123456789012:role/platform-admin\" Viewing RBAC bindings kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.subjects[0].kind==\\\"Group\\\") | .metadata.name' kubectl get rolebindings -A -o json | jq -r '.items[] | select(.subjects[0].kind==\\\"Group\\\") | .metadata.name' Example cluster role binding: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: my-cluster-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:discovery subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: my-group This grants members of my-group the permissions of system:discovery across all namespaces. Summary Use aws eks update-kubeconfig with cluster outputs to get access. Add IAM admins via admin_arns on aws_k8s_base (maps to system:masters ). For custom RBAC, edit aws-auth or create your own role/cluster role bindings.","title":"EKS Access"},{"location":"references/aws_eks_access/#aws-eks-access","text":"How to access EKS clusters generated by pltf.","title":"AWS EKS Access"},{"location":"references/aws_eks_access/#kubeconfig","text":"Fetch outputs: bash pltf terraform output -f env.yaml -e <env> --json | jq '.aws_eks' Note k8s_cluster_name , k8s_endpoint , and k8s_ca_data . Update kubeconfig: bash aws eks update-kubeconfig \\ --region <region> \\ --name <cluster> \\ --profile <aws-profile-if-needed> Use the same AWS profile that has access to the environment account (or backend profile if you share credentials). Generated Terraform already configures Kubernetes and Helm providers using these outputs when you run pltf terraform plan/apply .","title":"Kubeconfig"},{"location":"references/aws_eks_access/#aws-iam-to-kubernetes-rbac","text":"EKS uses the aws-auth ConfigMap in kube-system to map IAM users/roles to Kubernetes groups. Example aws-auth data: apiVersion: v1 data: mapRoles: | - groups: ['system:bootstrappers', 'system:nodes'] rolearn: arn:aws:iam::ACCOUNT_ID:role/pltf-live-example-dev-eks-default-node-group username: system:node:{{EC2PrivateDNSName}} - groups: ['system:masters'] rolearn: arn:aws:iam::ACCOUNT_ID:role/demo-admin username: pltf-managed mapUsers: | - groups: ['system:masters'] userarn: arn:aws:iam::ACCOUNT_ID:user/demo-admin username: pltf-managed Fields: - rolearn / userarn : IAM principal. - username : friendly alias. - groups : Kubernetes RBAC groups (use system:masters for admin).","title":"AWS IAM to Kubernetes RBAC"},{"location":"references/aws_eks_access/#granting-access-via-pltf","text":"Use admin_arns on aws_k8s_base to inject IAM admins without editing Kubernetes directly: modules: - type: aws_k8s_base admin_arns: - \"arn:aws:iam::123456789012:user/platform-admin\" - \"arn:aws:iam::123456789012:role/platform-admin\"","title":"Granting access via pltf"},{"location":"references/aws_eks_access/#viewing-rbac-bindings","text":"kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.subjects[0].kind==\\\"Group\\\") | .metadata.name' kubectl get rolebindings -A -o json | jq -r '.items[] | select(.subjects[0].kind==\\\"Group\\\") | .metadata.name' Example cluster role binding: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: my-cluster-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:discovery subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: my-group This grants members of my-group the permissions of system:discovery across all namespaces.","title":"Viewing RBAC bindings"},{"location":"references/aws_eks_access/#summary","text":"Use aws eks update-kubeconfig with cluster outputs to get access. Add IAM admins via admin_arns on aws_k8s_base (maps to system:masters ). For custom RBAC, edit aws-auth or create your own role/cluster role bindings.","title":"Summary"},{"location":"references/aws_eks_upgrade/","text":"AWS EKS Upgrade How to upgrade the version of your EKS cluster created by pltf. Overview EKS does not auto-upgrade clusters. Upgrade one minor version at a time (e.g., 1.24 \u2192 1.25). Steps below use the AWS console; CLI works too. Step 1: Control Plane Open the EKS cluster (correct region). Click Update now on the control plane. Select the next Kubernetes version and start the update. Important: During control plane upgrade (~20 min) avoid new deploys or kubectl changes. Running workloads keep serving traffic. Step 2: Node Groups Go to Configuration \u2192 Compute . For each managed node group, click Update now . Use Rolling update strategy and start the upgrade. Important: Nodes are replaced. If ingress is not HA, expect brief downtime while pods reschedule. Upgrade node groups one at a time. Step 3: Pin versions in specs After upgrading, pin the new versions so future applies stay consistent: modules: - type: aws_eks k8s_version: \"1.25\" - type: aws_nodegroup name: default k8s_version: \"1.25\" Then: pltf terraform plan -f env.yaml -e prod pltf terraform apply -f env.yaml -e prod Breaking Changes Review Kubernetes API deprecations for your target version. Ensure add-ons (CNI, metrics, ingress) are compatible. For multi-hop upgrades (e.g., 1.22 \u2192 1.24), step through each minor version. References AWS: https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html Versions: https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html","title":"EKS Upgrade"},{"location":"references/aws_eks_upgrade/#aws-eks-upgrade","text":"How to upgrade the version of your EKS cluster created by pltf.","title":"AWS EKS Upgrade"},{"location":"references/aws_eks_upgrade/#overview","text":"EKS does not auto-upgrade clusters. Upgrade one minor version at a time (e.g., 1.24 \u2192 1.25). Steps below use the AWS console; CLI works too.","title":"Overview"},{"location":"references/aws_eks_upgrade/#step-1-control-plane","text":"Open the EKS cluster (correct region). Click Update now on the control plane. Select the next Kubernetes version and start the update. Important: During control plane upgrade (~20 min) avoid new deploys or kubectl changes. Running workloads keep serving traffic.","title":"Step 1: Control Plane"},{"location":"references/aws_eks_upgrade/#step-2-node-groups","text":"Go to Configuration \u2192 Compute . For each managed node group, click Update now . Use Rolling update strategy and start the upgrade. Important: Nodes are replaced. If ingress is not HA, expect brief downtime while pods reschedule. Upgrade node groups one at a time.","title":"Step 2: Node Groups"},{"location":"references/aws_eks_upgrade/#step-3-pin-versions-in-specs","text":"After upgrading, pin the new versions so future applies stay consistent: modules: - type: aws_eks k8s_version: \"1.25\" - type: aws_nodegroup name: default k8s_version: \"1.25\" Then: pltf terraform plan -f env.yaml -e prod pltf terraform apply -f env.yaml -e prod","title":"Step 3: Pin versions in specs"},{"location":"references/aws_eks_upgrade/#breaking-changes","text":"Review Kubernetes API deprecations for your target version. Ensure add-ons (CNI, metrics, ingress) are compatible. For multi-hop upgrades (e.g., 1.22 \u2192 1.24), step through each minor version.","title":"Breaking Changes"},{"location":"references/aws_eks_upgrade/#references","text":"AWS: https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html Versions: https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html","title":"References"},{"location":"references/azure/","text":"Azure Reference Azure Reference Placeholder. Document provider settings, backend options (azurerm/s3/gcs), and guidance for custom modules or future Azure modules.","title":"Azure Reference"},{"location":"references/azure/#azure-reference","text":"","title":"Azure Reference"},{"location":"references/azure/#azure-reference_1","text":"Placeholder. Document provider settings, backend options (azurerm/s3/gcs), and guidance for custom modules or future Azure modules.","title":"Azure Reference"},{"location":"references/gcp/","text":"GCP Reference Placeholder. Document provider settings, backend options (gcs or s3), and guidance for custom modules.","title":"GCP Reference"},{"location":"references/gcp/#gcp-reference","text":"Placeholder. Document provider settings, backend options (gcs or s3), and guidance for custom modules.","title":"GCP Reference"},{"location":"references/modules/aws_base/","text":"aws_base Provision networking (VPC), subnets across AZs, flow logs, NAT, and a default KMS key + log bucket for the environment. What it does Creates a new VPC (or imports an existing one) with public/private subnets across three AZs. Adds internet/NAT gateways and route tables for public/private egress. Enables VPC flow logs to the log bucket and provisions a default KMS key. Creates a log bucket for access/flow logs used by other modules. Fields Name Description Default Required private_ipv4_cidr_blocks Cidr blocks for private subnets. One for each desired AZ ['10.0.128.0/21', '10.0.136.0/21', '10.0.144.0/21'] False private_subnet_ids List of pre-existing private subnets to use instead of creating new subnets for pltf. Required when var.vpc_id is set. False public_ipv4_cidr_blocks Cidr blocks for public subnets. One for each desired AZ ['10.0.0.0/21', '10.0.8.0/21', '10.0.16.0/21'] False public_subnet_ids List of pre-existing public subnets to use instead of creating new subnets for pltf. Required when var.vpc_id is set. False total_ipv4_cidr_block Cidr block to reserve for whole vpc 10.0.0.0/16 False vpc_id The ID of an pre-existing VPC to use instead of creating a new VPC for pltf False vpc_log_retention 90 False Bring your own VPC To use an existing VPC, set vpc_id , public_subnet_ids , and private_subnet_ids . Public subnets must route to an internet gateway and assign public IPs. Private subnets must route 0.0.0.0/0 to a NAT gateway with a public IP. Misconfigured routes may yield Terraform errors like \"No routes matching supplied arguments found in Route Table\". IPv6 imports are not validated; dual-stack may work but is not verified. Outputs Name Description kms_account_key_arn ARN of the default KMS key for environment resources kms_account_key_id ID of the default KMS key private_subnet_ids Private subnet IDs provisioned/imported public_nat_ips Elastic IPs of NAT gateways public_subnets_ids Public subnet IDs provisioned/imported s3_log_bucket_name Name of the environment log bucket vpc_id VPC ID provisioned/imported","title":"aws_base"},{"location":"references/modules/aws_base/#aws_base","text":"Provision networking (VPC), subnets across AZs, flow logs, NAT, and a default KMS key + log bucket for the environment.","title":"aws_base"},{"location":"references/modules/aws_base/#what-it-does","text":"Creates a new VPC (or imports an existing one) with public/private subnets across three AZs. Adds internet/NAT gateways and route tables for public/private egress. Enables VPC flow logs to the log bucket and provisions a default KMS key. Creates a log bucket for access/flow logs used by other modules.","title":"What it does"},{"location":"references/modules/aws_base/#fields","text":"Name Description Default Required private_ipv4_cidr_blocks Cidr blocks for private subnets. One for each desired AZ ['10.0.128.0/21', '10.0.136.0/21', '10.0.144.0/21'] False private_subnet_ids List of pre-existing private subnets to use instead of creating new subnets for pltf. Required when var.vpc_id is set. False public_ipv4_cidr_blocks Cidr blocks for public subnets. One for each desired AZ ['10.0.0.0/21', '10.0.8.0/21', '10.0.16.0/21'] False public_subnet_ids List of pre-existing public subnets to use instead of creating new subnets for pltf. Required when var.vpc_id is set. False total_ipv4_cidr_block Cidr block to reserve for whole vpc 10.0.0.0/16 False vpc_id The ID of an pre-existing VPC to use instead of creating a new VPC for pltf False vpc_log_retention 90 False","title":"Fields"},{"location":"references/modules/aws_base/#bring-your-own-vpc","text":"To use an existing VPC, set vpc_id , public_subnet_ids , and private_subnet_ids . Public subnets must route to an internet gateway and assign public IPs. Private subnets must route 0.0.0.0/0 to a NAT gateway with a public IP. Misconfigured routes may yield Terraform errors like \"No routes matching supplied arguments found in Route Table\". IPv6 imports are not validated; dual-stack may work but is not verified.","title":"Bring your own VPC"},{"location":"references/modules/aws_base/#outputs","text":"Name Description kms_account_key_arn ARN of the default KMS key for environment resources kms_account_key_id ID of the default KMS key private_subnet_ids Private subnet IDs provisioned/imported public_nat_ips Elastic IPs of NAT gateways public_subnets_ids Public subnet IDs provisioned/imported s3_log_bucket_name Name of the environment log bucket vpc_id VPC ID provisioned/imported","title":"Outputs"},{"location":"references/modules/aws_dns/","text":"aws_dns Creates a Route53 hosted zone and ACM certificate with DNS validation, wiring records for ingress/load balancers. What it does Creates Route53 hosted zone and ACM cert with DNS validation or import. Exposes NS records and cert ARN for downstream modules. Fields Name Description Default Required cert_chain_included False False delegated False False domain True external_cert_arn False force_update False False upload_cert False False Outputs Name Description cert_arn ACM certificate ARN (created/imported/external). domain Domain name of the hosted zone. name_servers Delegated name servers. zone_id Route53 hosted zone ID.","title":"aws_dns"},{"location":"references/modules/aws_dns/#aws_dns","text":"Creates a Route53 hosted zone and ACM certificate with DNS validation, wiring records for ingress/load balancers.","title":"aws_dns"},{"location":"references/modules/aws_dns/#what-it-does","text":"Creates Route53 hosted zone and ACM cert with DNS validation or import. Exposes NS records and cert ARN for downstream modules.","title":"What it does"},{"location":"references/modules/aws_dns/#fields","text":"Name Description Default Required cert_chain_included False False delegated False False domain True external_cert_arn False force_update False False upload_cert False False","title":"Fields"},{"location":"references/modules/aws_dns/#outputs","text":"Name Description cert_arn ACM certificate ARN (created/imported/external). domain Domain name of the hosted zone. name_servers Delegated name servers. zone_id Route53 hosted zone ID.","title":"Outputs"},{"location":"references/modules/aws_documentdb/","text":"aws_documentdb Provision a DocumentDB cluster with subnet groups, encryption, backups, and security groups in the base VPC. What it does Creates a DocumentDB cluster with configurable engine version and instance count. Uses subnet groups and security groups from the VPC; enables encryption. Supports deletion protection and exposes host/user/password outputs. Fields Name Description Default Required deletion_protection A value that indicates whether the DB cluster has deletion protection enabled. The database can't be deleted when deletion protection is enabled. False False engine_version 4.0.0 False instance_class db.r5.large False instance_count Number of Instances for aws_docdb_cluster_instance 1 False Outputs Name Description db_host Cluster endpoint. db_password Master password. db_user Master username.","title":"aws_documentdb"},{"location":"references/modules/aws_documentdb/#aws_documentdb","text":"Provision a DocumentDB cluster with subnet groups, encryption, backups, and security groups in the base VPC.","title":"aws_documentdb"},{"location":"references/modules/aws_documentdb/#what-it-does","text":"Creates a DocumentDB cluster with configurable engine version and instance count. Uses subnet groups and security groups from the VPC; enables encryption. Supports deletion protection and exposes host/user/password outputs.","title":"What it does"},{"location":"references/modules/aws_documentdb/#fields","text":"Name Description Default Required deletion_protection A value that indicates whether the DB cluster has deletion protection enabled. The database can't be deleted when deletion protection is enabled. False False engine_version 4.0.0 False instance_class db.r5.large False instance_count Number of Instances for aws_docdb_cluster_instance 1 False","title":"Fields"},{"location":"references/modules/aws_documentdb/#outputs","text":"Name Description db_host Cluster endpoint. db_password Master password. db_user Master username.","title":"Outputs"},{"location":"references/modules/aws_dynamodb/","text":"aws_dynamodb Creates a DynamoDB table with encryption, throughput settings, TTL, and optional point-in-time recovery. What it does Creates a DynamoDB table with server-side encryption and customizable billing mode. Supports provisioned throughput settings and TTL via attributes. Exposes table ARN/ID and KMS key details. Fields Name Description Default Required attributes True billing_mode PROVISIONED False hash_key False range_key False read_capacity 20 False write_capacity 20 False Outputs Name Description kms_arn KMS key ARN used for encryption. kms_id KMS key ID used for encryption. table_arn Table ARN. table_id Table name/ID.","title":"aws_dynamodb"},{"location":"references/modules/aws_dynamodb/#aws_dynamodb","text":"Creates a DynamoDB table with encryption, throughput settings, TTL, and optional point-in-time recovery.","title":"aws_dynamodb"},{"location":"references/modules/aws_dynamodb/#what-it-does","text":"Creates a DynamoDB table with server-side encryption and customizable billing mode. Supports provisioned throughput settings and TTL via attributes. Exposes table ARN/ID and KMS key details.","title":"What it does"},{"location":"references/modules/aws_dynamodb/#fields","text":"Name Description Default Required attributes True billing_mode PROVISIONED False hash_key False range_key False read_capacity 20 False write_capacity 20 False","title":"Fields"},{"location":"references/modules/aws_dynamodb/#outputs","text":"Name Description kms_arn KMS key ARN used for encryption. kms_id KMS key ID used for encryption. table_arn Table ARN. table_id Table name/ID.","title":"Outputs"},{"location":"references/modules/aws_eks/","text":"aws_eks Creates an EKS control plane in private subnets, configurable Kubernetes version, logging, and OIDC provider for IRSA. What it does Provisions the EKS control plane in private subnets with security groups. Creates an OIDC provider for IRSA and enables control-plane logging. Stands up a default managed nodegroup with scaling/min/max and optional spot. Fields Name Description Default Required ami_type AL2023_x86_64_STANDARD False cluster_name True control_plane_security_groups List of security groups to give control plane access to [] False eks_log_retention 7 False enable_metrics True k8s_version 1.21 False kms_account_key_arn True max_nodes 5 False min_nodes 3 False node_disk_size 20 False node_instance_type t3.medium False node_launch_template {} False private_subnet_ids True spot_instances False False vpc_id True Outputs Name Description k8s_ca_data Cluster CA data (base64). k8s_cluster_name EKS cluster name. k8s_endpoint EKS API endpoint. k8s_node_group_security_id Security group ID for nodes. k8s_openid_provider_arn OIDC provider ARN for IRSA. k8s_openid_provider_url OIDC provider URL. k8s_version Kubernetes version.","title":"aws_eks"},{"location":"references/modules/aws_eks/#aws_eks","text":"Creates an EKS control plane in private subnets, configurable Kubernetes version, logging, and OIDC provider for IRSA.","title":"aws_eks"},{"location":"references/modules/aws_eks/#what-it-does","text":"Provisions the EKS control plane in private subnets with security groups. Creates an OIDC provider for IRSA and enables control-plane logging. Stands up a default managed nodegroup with scaling/min/max and optional spot.","title":"What it does"},{"location":"references/modules/aws_eks/#fields","text":"Name Description Default Required ami_type AL2023_x86_64_STANDARD False cluster_name True control_plane_security_groups List of security groups to give control plane access to [] False eks_log_retention 7 False enable_metrics True k8s_version 1.21 False kms_account_key_arn True max_nodes 5 False min_nodes 3 False node_disk_size 20 False node_instance_type t3.medium False node_launch_template {} False private_subnet_ids True spot_instances False False vpc_id True","title":"Fields"},{"location":"references/modules/aws_eks/#outputs","text":"Name Description k8s_ca_data Cluster CA data (base64). k8s_cluster_name EKS cluster name. k8s_endpoint EKS API endpoint. k8s_node_group_security_id Security group ID for nodes. k8s_openid_provider_arn OIDC provider ARN for IRSA. k8s_openid_provider_url OIDC provider URL. k8s_version Kubernetes version.","title":"Outputs"},{"location":"references/modules/aws_iam_policy/","text":"aws_iam_policy Defines IAM policies (inline or managed) to attach to roles/users created by other modules. What it does Creates a standalone IAM policy from a JSON document for reuse. Fields Name Description Default Required file Json file path containing the Policy False Outputs Name Description policy_arn IAM policy ARN. policy_id IAM policy ID. policy_name IAM policy name.","title":"aws_iam_policy"},{"location":"references/modules/aws_iam_policy/#aws_iam_policy","text":"Defines IAM policies (inline or managed) to attach to roles/users created by other modules.","title":"aws_iam_policy"},{"location":"references/modules/aws_iam_policy/#what-it-does","text":"Creates a standalone IAM policy from a JSON document for reuse.","title":"What it does"},{"location":"references/modules/aws_iam_policy/#fields","text":"Name Description Default Required file Json file path containing the Policy False","title":"Fields"},{"location":"references/modules/aws_iam_policy/#outputs","text":"Name Description policy_arn IAM policy ARN. policy_id IAM policy ID. policy_name IAM policy name.","title":"Outputs"},{"location":"references/modules/aws_iam_role/","text":"aws_iam_role Creates IAM roles with inline/managed policies and OIDC trust for Kubernetes service accounts (IRSA). What it does Creates an IAM role with inline policy and optional managed policies. Supports IRSA/OIDC trust for Kubernetes service accounts and trust for other IAM principals. Auto-generates least-privilege policies from links when used with supported modules. Fields Name Description Default Required allowed_iams [] False allowed_k8s_services [] False extra_iam_policies [] False iam_policy True kubernetes_trusts [] False links [] False Outputs Name Description role_arn IAM role ARN.","title":"aws_iam_role"},{"location":"references/modules/aws_iam_role/#aws_iam_role","text":"Creates IAM roles with inline/managed policies and OIDC trust for Kubernetes service accounts (IRSA).","title":"aws_iam_role"},{"location":"references/modules/aws_iam_role/#what-it-does","text":"Creates an IAM role with inline policy and optional managed policies. Supports IRSA/OIDC trust for Kubernetes service accounts and trust for other IAM principals. Auto-generates least-privilege policies from links when used with supported modules.","title":"What it does"},{"location":"references/modules/aws_iam_role/#fields","text":"Name Description Default Required allowed_iams [] False allowed_k8s_services [] False extra_iam_policies [] False iam_policy True kubernetes_trusts [] False links [] False","title":"Fields"},{"location":"references/modules/aws_iam_role/#outputs","text":"Name Description role_arn IAM role ARN.","title":"Outputs"},{"location":"references/modules/aws_iam_user/","text":"aws_iam_user Creates IAM users with optional access keys and managed/inline policy attachments. What it does Creates an IAM user with inline/managed policies. Optionally auto-generates policies from links and returns access keys if created. Fields Name Description Default Required extra_iam_policies [] False iam_policy True links [] False Outputs Name Description user_arn IAM user ARN.","title":"aws_iam_user"},{"location":"references/modules/aws_iam_user/#aws_iam_user","text":"Creates IAM users with optional access keys and managed/inline policy attachments.","title":"aws_iam_user"},{"location":"references/modules/aws_iam_user/#what-it-does","text":"Creates an IAM user with inline/managed policies. Optionally auto-generates policies from links and returns access keys if created.","title":"What it does"},{"location":"references/modules/aws_iam_user/#fields","text":"Name Description Default Required extra_iam_policies [] False iam_policy True links [] False","title":"Fields"},{"location":"references/modules/aws_iam_user/#outputs","text":"Name Description user_arn IAM user ARN.","title":"Outputs"},{"location":"references/modules/aws_k8s_base/","text":"aws_k8s_base Bootstraps core Kubernetes add-ons on EKS: ingress, autoscaler, metrics server, external-dns, and optional admins. What it does Deploys ingress-nginx (optionally HA) and configures extra TCP ports if set. Deploys external-dns, cert-manager, metrics-server, and cluster-autoscaler. Optionally deploys Linkerd service mesh and grants admin access via admin_arns. Wires ingress/records to the provided domain and hosted zone. Fields Name Description Default Required admin_arns [] False cert_arn False cert_manager_values {} False certificate_body False certificate_chain False domain True eks_cluster_name True enable_auto_dns True expose_self_signed_ssl False False ingress_nginx_values {} False k8s_cluster_name True k8s_version True linkerd_enabled True False linkerd_high_availability False False linkerd_values {} False nginx_config {} False nginx_enabled True nginx_extra_tcp_ports {} False nginx_extra_tcp_ports_tls [] False nginx_high_availability False False openid_provider_arn True openid_provider_url True private_key False s3_log_bucket_name True zone_id True Outputs Name Description load_balancer_arn Ingress load balancer ARN. load_balancer_raw_dns Ingress load balancer DNS name.","title":"aws_k8s_base"},{"location":"references/modules/aws_k8s_base/#aws_k8s_base","text":"Bootstraps core Kubernetes add-ons on EKS: ingress, autoscaler, metrics server, external-dns, and optional admins.","title":"aws_k8s_base"},{"location":"references/modules/aws_k8s_base/#what-it-does","text":"Deploys ingress-nginx (optionally HA) and configures extra TCP ports if set. Deploys external-dns, cert-manager, metrics-server, and cluster-autoscaler. Optionally deploys Linkerd service mesh and grants admin access via admin_arns. Wires ingress/records to the provided domain and hosted zone.","title":"What it does"},{"location":"references/modules/aws_k8s_base/#fields","text":"Name Description Default Required admin_arns [] False cert_arn False cert_manager_values {} False certificate_body False certificate_chain False domain True eks_cluster_name True enable_auto_dns True expose_self_signed_ssl False False ingress_nginx_values {} False k8s_cluster_name True k8s_version True linkerd_enabled True False linkerd_high_availability False False linkerd_values {} False nginx_config {} False nginx_enabled True nginx_extra_tcp_ports {} False nginx_extra_tcp_ports_tls [] False nginx_high_availability False False openid_provider_arn True openid_provider_url True private_key False s3_log_bucket_name True zone_id True","title":"Fields"},{"location":"references/modules/aws_k8s_base/#outputs","text":"Name Description load_balancer_arn Ingress load balancer ARN. load_balancer_raw_dns Ingress load balancer DNS name.","title":"Outputs"},{"location":"references/modules/aws_k8s_service/","text":"aws_k8s_service Deploys a Kubernetes workload via Helm with service/HPA/ingress and IRSA wiring; links drive IAM policies. What it does Creates a Helm release with Deployment/Service/HPA and optional Ingress. Configures service account + IRSA using provided OIDC info and IAM policy. Supports env vars, secrets, cronjobs, persistent volumes, and pod annotations. Supports sticky sessions, custom probes, ingress annotations, and extra IAM policies. Fields Name Description Default Required additional_iam_policies [] False args True autoscaling_target_cpu_percentage Percentage of requested cpu after which autoscaling kicks in 80 False autoscaling_target_mem_percentage Percentage of requested memory after which autoscaling kicks in 80 False commands True consistent_hash False cron_jobs [] False digest Digest of image to be deployed False domain False env_vars Environment variables to pass to the container [] False healthcheck_command True healthcheck_path False http_port The port that exposes an HTTP interface False iam_policy True image External Image to be deployed True ingress_extra_annotations {} False initial_liveness_delay 30 False initial_readiness_delay 30 False keep_path_prefix False False link_secrets [] False links False liveness_probe_command True liveness_probe_path Url path for liveness probe False max_containers Max value for HPA autoscaling 3 False max_history True min_containers Min value for HPA autoscaling 1 False openid_provider_arn True openid_provider_url True persistent_storage [] False pod_annotations values to add to the pod annotations for the k8s-service pods {} False pod_labels True ports Ports to be exposed True probe_port The port that is used for health probes False public_uri [] False readiness_probe_command True readiness_probe_path Url path for readiness probe False resource_limits True resource_request {'cpu': 100, 'memory': 128} False secrets False service_annotations Annotations to add to the service resource {} False sticky_session False False sticky_session_max_age 86400 False tag Tag of image to be deployed False timeout 300 False tolerations [] False Outputs Name Description current_digest Current image digest deployed. current_image current_tag docker_repo_url","title":"aws_k8s_service"},{"location":"references/modules/aws_k8s_service/#aws_k8s_service","text":"Deploys a Kubernetes workload via Helm with service/HPA/ingress and IRSA wiring; links drive IAM policies.","title":"aws_k8s_service"},{"location":"references/modules/aws_k8s_service/#what-it-does","text":"Creates a Helm release with Deployment/Service/HPA and optional Ingress. Configures service account + IRSA using provided OIDC info and IAM policy. Supports env vars, secrets, cronjobs, persistent volumes, and pod annotations. Supports sticky sessions, custom probes, ingress annotations, and extra IAM policies.","title":"What it does"},{"location":"references/modules/aws_k8s_service/#fields","text":"Name Description Default Required additional_iam_policies [] False args True autoscaling_target_cpu_percentage Percentage of requested cpu after which autoscaling kicks in 80 False autoscaling_target_mem_percentage Percentage of requested memory after which autoscaling kicks in 80 False commands True consistent_hash False cron_jobs [] False digest Digest of image to be deployed False domain False env_vars Environment variables to pass to the container [] False healthcheck_command True healthcheck_path False http_port The port that exposes an HTTP interface False iam_policy True image External Image to be deployed True ingress_extra_annotations {} False initial_liveness_delay 30 False initial_readiness_delay 30 False keep_path_prefix False False link_secrets [] False links False liveness_probe_command True liveness_probe_path Url path for liveness probe False max_containers Max value for HPA autoscaling 3 False max_history True min_containers Min value for HPA autoscaling 1 False openid_provider_arn True openid_provider_url True persistent_storage [] False pod_annotations values to add to the pod annotations for the k8s-service pods {} False pod_labels True ports Ports to be exposed True probe_port The port that is used for health probes False public_uri [] False readiness_probe_command True readiness_probe_path Url path for readiness probe False resource_limits True resource_request {'cpu': 100, 'memory': 128} False secrets False service_annotations Annotations to add to the service resource {} False sticky_session False False sticky_session_max_age 86400 False tag Tag of image to be deployed False timeout 300 False tolerations [] False","title":"Fields"},{"location":"references/modules/aws_k8s_service/#outputs","text":"Name Description current_digest Current image digest deployed. current_image current_tag docker_repo_url","title":"Outputs"},{"location":"references/modules/aws_mysql/","text":"aws_mysql Provisions an Aurora MySQL cluster with subnet group, encryption, backups, and optional multi-AZ. What it does Provisions an Aurora MySQL cluster in private subnets with encryption. Supports multi-AZ, backups, retention, and public accessibility toggle. Exposes writer/reader endpoints and security/subnet group metadata. Fields Name Description Default Required backup_retention_days How many days to keep the backup retention True db_name app False engine_version 5.7.mysql_aurora.2.04.2 False instance_class db.t3.medium False multi_az False False safety False False Outputs Name Description db_host db_name db_password db_user","title":"aws_mysql"},{"location":"references/modules/aws_mysql/#aws_mysql","text":"Provisions an Aurora MySQL cluster with subnet group, encryption, backups, and optional multi-AZ.","title":"aws_mysql"},{"location":"references/modules/aws_mysql/#what-it-does","text":"Provisions an Aurora MySQL cluster in private subnets with encryption. Supports multi-AZ, backups, retention, and public accessibility toggle. Exposes writer/reader endpoints and security/subnet group metadata.","title":"What it does"},{"location":"references/modules/aws_mysql/#fields","text":"Name Description Default Required backup_retention_days How many days to keep the backup retention True db_name app False engine_version 5.7.mysql_aurora.2.04.2 False instance_class db.t3.medium False multi_az False False safety False False","title":"Fields"},{"location":"references/modules/aws_mysql/#outputs","text":"Name Description db_host db_name db_password db_user","title":"Outputs"},{"location":"references/modules/aws_nodegroup/","text":"aws_nodegroup Managed node group for EKS with scaling limits, instance type, disk size, and optional spot instances. What it does Adds an EKS managed node group with scaling limits and instance type/disk controls. Supports spot instances and custom labels/taints (via launch template inputs if set). Fields Name Description Default Required ami_type AL2023_x86_64_STANDARD False autoscaling_tags {} False labels {} False max_nodes 15 False min_nodes 3 False node_disk_size 20 False node_instance_type t3.medium False spot_instances False False taints [] False use_gpu False False Outputs Name Description","title":"aws_nodegroup"},{"location":"references/modules/aws_nodegroup/#aws_nodegroup","text":"Managed node group for EKS with scaling limits, instance type, disk size, and optional spot instances.","title":"aws_nodegroup"},{"location":"references/modules/aws_nodegroup/#what-it-does","text":"Adds an EKS managed node group with scaling limits and instance type/disk controls. Supports spot instances and custom labels/taints (via launch template inputs if set).","title":"What it does"},{"location":"references/modules/aws_nodegroup/#fields","text":"Name Description Default Required ami_type AL2023_x86_64_STANDARD False autoscaling_tags {} False labels {} False max_nodes 15 False min_nodes 3 False node_disk_size 20 False node_instance_type t3.medium False spot_instances False False taints [] False use_gpu False False","title":"Fields"},{"location":"references/modules/aws_nodegroup/#outputs","text":"Name Description","title":"Outputs"},{"location":"references/modules/aws_postgres/","text":"aws_postgres Provisions an Aurora Postgres cluster with subnet group, encryption, backups, and optional multi-AZ. What it does Provisions an Aurora Postgres cluster in private subnets with encryption. Supports multi-AZ, backups, retention, and public accessibility toggle. Exposes writer/reader endpoints and security/subnet group metadata. Fields Name Description Default Required backup_retention_days How many days to keep the backup retention True create_global_database True database_name True engine_version 11.9 False existing_global_database_id True extra_security_groups_ids True instance_class db.t3.medium False multi_az False False restore_from_snapshot True safety False False Outputs Name Description db_host db_name db_password db_user global_database_id","title":"aws_postgres"},{"location":"references/modules/aws_postgres/#aws_postgres","text":"Provisions an Aurora Postgres cluster with subnet group, encryption, backups, and optional multi-AZ.","title":"aws_postgres"},{"location":"references/modules/aws_postgres/#what-it-does","text":"Provisions an Aurora Postgres cluster in private subnets with encryption. Supports multi-AZ, backups, retention, and public accessibility toggle. Exposes writer/reader endpoints and security/subnet group metadata.","title":"What it does"},{"location":"references/modules/aws_postgres/#fields","text":"Name Description Default Required backup_retention_days How many days to keep the backup retention True create_global_database True database_name True engine_version 11.9 False existing_global_database_id True extra_security_groups_ids True instance_class db.t3.medium False multi_az False False restore_from_snapshot True safety False False","title":"Fields"},{"location":"references/modules/aws_postgres/#outputs","text":"Name Description db_host db_name db_password db_user global_database_id","title":"Outputs"},{"location":"references/modules/aws_redis/","text":"aws_redis Provisions ElastiCache Redis with subnet group, encryption in-transit/at-rest, and parameter options. What it does Deploys ElastiCache Redis cluster/subnet group with in-transit/at-rest encryption. Configurable engine version, node class, cluster size, and parameter family. Outputs cache endpoints and security group details. Fields Name Description Default Required node_type cache.m4.large False redis_version 6.x False snapshot_retention_limit Days for which the Snapshot should be retained. 0 False snapshot_window When should the Snapshot for redis cache be done. UTC Time. Snapshot Retention Limit should be set to more than 0. 04:00-05:00 False Outputs Name Description cache_auth_token cache_host Redis host.","title":"aws_redis"},{"location":"references/modules/aws_redis/#aws_redis","text":"Provisions ElastiCache Redis with subnet group, encryption in-transit/at-rest, and parameter options.","title":"aws_redis"},{"location":"references/modules/aws_redis/#what-it-does","text":"Deploys ElastiCache Redis cluster/subnet group with in-transit/at-rest encryption. Configurable engine version, node class, cluster size, and parameter family. Outputs cache endpoints and security group details.","title":"What it does"},{"location":"references/modules/aws_redis/#fields","text":"Name Description Default Required node_type cache.m4.large False redis_version 6.x False snapshot_retention_limit Days for which the Snapshot should be retained. 0 False snapshot_window When should the Snapshot for redis cache be done. UTC Time. Snapshot Retention Limit should be set to more than 0. 04:00-05:00 False","title":"Fields"},{"location":"references/modules/aws_redis/#outputs","text":"Name Description cache_auth_token cache_host Redis host.","title":"Outputs"},{"location":"references/modules/aws_s3/","text":"aws_s3 Creates an S3 bucket with encryption, versioning, lifecycle/replication options, and optional bucket policies. What it does Creates an encrypted S3 bucket (AES-256) with block-public-access by default. Supports custom bucket policy, CORS rules, and optional same-region replication. Optionally uploads static files with content-type detection and creates an OAI for CloudFront reads when needed. Can emit access logs to the provided log bucket. Fields Name Description Default Required block_public True False bucket_name True bucket_policy False cors_rule CORS configuration for the bucket. False files False s3_log_bucket_name False same_region_replication False False Outputs Name Description bucket_arn Bucket ARN. bucket_id Bucket name/ID. cloudfront_read_path Origin access identity path (if created).","title":"aws_s3"},{"location":"references/modules/aws_s3/#aws_s3","text":"Creates an S3 bucket with encryption, versioning, lifecycle/replication options, and optional bucket policies.","title":"aws_s3"},{"location":"references/modules/aws_s3/#what-it-does","text":"Creates an encrypted S3 bucket (AES-256) with block-public-access by default. Supports custom bucket policy, CORS rules, and optional same-region replication. Optionally uploads static files with content-type detection and creates an OAI for CloudFront reads when needed. Can emit access logs to the provided log bucket.","title":"What it does"},{"location":"references/modules/aws_s3/#fields","text":"Name Description Default Required block_public True False bucket_name True bucket_policy False cors_rule CORS configuration for the bucket. False files False s3_log_bucket_name False same_region_replication False False","title":"Fields"},{"location":"references/modules/aws_s3/#outputs","text":"Name Description bucket_arn Bucket ARN. bucket_id Bucket name/ID. cloudfront_read_path Origin access identity path (if created).","title":"Outputs"},{"location":"references/modules/aws_ses/","text":"aws_ses Configures SES domain/identities with DNS verification records and optional inbound/notification settings. What it does Verifies a domain in SES and configures MAIL FROM. Creates IAM policy for sending and exposes DKIM tokens and identity ARN. Fields Name Description Default Required domain True mail_from_prefix mail False zone_id True Outputs Name Description identity_arn SES identity ARN. sender_policy_arn IAM policy ARN permitting SES send.","title":"aws_ses"},{"location":"references/modules/aws_ses/#aws_ses","text":"Configures SES domain/identities with DNS verification records and optional inbound/notification settings.","title":"aws_ses"},{"location":"references/modules/aws_ses/#what-it-does","text":"Verifies a domain in SES and configures MAIL FROM. Creates IAM policy for sending and exposes DKIM tokens and identity ARN.","title":"What it does"},{"location":"references/modules/aws_ses/#fields","text":"Name Description Default Required domain True mail_from_prefix mail False zone_id True","title":"Fields"},{"location":"references/modules/aws_ses/#outputs","text":"Name Description identity_arn SES identity ARN. sender_policy_arn IAM policy ARN permitting SES send.","title":"Outputs"},{"location":"references/modules/aws_sns/","text":"aws_sns Creates an SNS topic with encryption, delivery policies, and optional subscriptions. What it does Creates an SNS topic (standard or FIFO) with a dedicated KMS CMK. Applies a default topic policy for account root and subscribes provided SQS endpoints. Supports content-based deduplication for FIFO topics and custom delivery policy. Fields Name Description Default Required content_based_deduplication Enable content-based deduplication for FIFO topics. False False fifo Create a FIFO topic (adds .fifo suffix). False False sqs_subscribers List of SQS queue ARNs to subscribe. [] False Outputs Name Description kms_arn KMS key ARN for the topic. topic_arn SNS topic ARN.","title":"aws_sns"},{"location":"references/modules/aws_sns/#aws_sns","text":"Creates an SNS topic with encryption, delivery policies, and optional subscriptions.","title":"aws_sns"},{"location":"references/modules/aws_sns/#what-it-does","text":"Creates an SNS topic (standard or FIFO) with a dedicated KMS CMK. Applies a default topic policy for account root and subscribes provided SQS endpoints. Supports content-based deduplication for FIFO topics and custom delivery policy.","title":"What it does"},{"location":"references/modules/aws_sns/#fields","text":"Name Description Default Required content_based_deduplication Enable content-based deduplication for FIFO topics. False False fifo Create a FIFO topic (adds .fifo suffix). False False sqs_subscribers List of SQS queue ARNs to subscribe. [] False","title":"Fields"},{"location":"references/modules/aws_sns/#outputs","text":"Name Description kms_arn KMS key ARN for the topic. topic_arn SNS topic ARN.","title":"Outputs"},{"location":"references/modules/aws_sqs/","text":"aws_sqs Creates an SQS queue with encryption, visibility timeout, redrive policy, and optional DLQ linkage. What it does Creates an SQS queue (standard or FIFO) with a dedicated KMS CMK. Configures default queue policy allowing account root, SNS, and EventBridge producers. Supports content-based deduplication, delivery delays, retention, and long polling. Outputs KMS ARN for wiring IRSA/IAM consumers. Fields Name Description Default Required content_based_deduplication Enable content-based deduplication for FIFO queues. False False delay_seconds 0 False fifo Create a FIFO queue (adds .fifo suffix). False False message_retention_seconds 345600 False receive_wait_time_seconds 0 False Outputs Name Description kms_arn KMS key ARN for the queue. queue_arn SQS queue ARN. queue_id SQS queue URL. queue_name SQS queue name.","title":"aws_sqs"},{"location":"references/modules/aws_sqs/#aws_sqs","text":"Creates an SQS queue with encryption, visibility timeout, redrive policy, and optional DLQ linkage.","title":"aws_sqs"},{"location":"references/modules/aws_sqs/#what-it-does","text":"Creates an SQS queue (standard or FIFO) with a dedicated KMS CMK. Configures default queue policy allowing account root, SNS, and EventBridge producers. Supports content-based deduplication, delivery delays, retention, and long polling. Outputs KMS ARN for wiring IRSA/IAM consumers.","title":"What it does"},{"location":"references/modules/aws_sqs/#fields","text":"Name Description Default Required content_based_deduplication Enable content-based deduplication for FIFO queues. False False delay_seconds 0 False fifo Create a FIFO queue (adds .fifo suffix). False False message_retention_seconds 345600 False receive_wait_time_seconds 0 False","title":"Fields"},{"location":"references/modules/aws_sqs/#outputs","text":"Name Description kms_arn KMS key ARN for the queue. queue_arn SQS queue ARN. queue_id SQS queue URL. queue_name SQS queue name.","title":"Outputs"},{"location":"references/modules/cloudfront_distribution/","text":"$m Placeholder. Add inputs, outputs, and usage notes for $m. What it does Creates a CloudFront distribution with configurable origins, cache behaviors, and TLS. Supports logging configuration and price class selection.","title":"cloudfront_distribution"},{"location":"references/modules/cloudfront_distribution/#m","text":"Placeholder. Add inputs, outputs, and usage notes for $m.","title":"$m"},{"location":"references/modules/cloudfront_distribution/#what-it-does","text":"Creates a CloudFront distribution with configurable origins, cache behaviors, and TLS. Supports logging configuration and price class selection.","title":"What it does"},{"location":"security/aws/","text":"AWS Architecture Architecture overview for AWS deployments of pltf. Description Single-region deployments with networking across three AZs by default (public + private subnets). Public subnets are used for public load balancers; EC2/Databases stay in private subnets (NAT for egress). EKS cluster spans private subnets with managed node groups. Cluster version is configurable ( aws_eks.k8s_version ) and patched by AWS. Public endpoint by default (VPN/private endpoints can be added later). Secrets are encrypted via KMS. Datastores: modules for Postgres (Aurora), Redis (ElastiCache), DocumentDB. Multi-AZ supported; 5-day backup retention for Postgres/DocumentDB. Credentials are generated and passed securely to services. S3: buckets are private by default, encrypted at rest (AES-256); can be made public via inputs. SQS: queues created with dedicated KMS keys for encryption at rest. SNS: topics created with dedicated KMS keys for encryption at rest. IAM: IAM role/user modules with links auto-generate least-privilege policies (S3, SQS, SNS, SES, etc.) and IRSA trusts for Kubernetes services. DNS/SSL: Route53 hosted zone and ACM certificates; validation via Route53; records created to point to the load balancer. Security Overview End-to-end TLS when using ingress + service mesh (Linkerd optional) and delegated domains. Databases and EC2s in private subnets; only NAT egress. Databases (Postgres/Redis/DocumentDB) encrypted at rest with KMS; connections use SSL. S3 buckets encrypted at rest (AES-256); private by default. SQS/SNS encrypted at rest with per-resource KMS keys. Networking gated by security groups (EKS-managed + module-specific SGs) with minimal port exposure. EKS nodes created with scoped IAM policies; cluster storage (Secrets) encrypted via KMS. K8s service accounts mapped to IAM roles via OIDC (IRSA); no long-lived credentials. No long-lived IAM credentials are created by default; ECR images remain private. 5-day backup retention for Postgres/DocumentDB. Public EKS endpoint by default for simplicity; private/VPN options can be layered later.","title":"AWS Architecture"},{"location":"security/aws/#aws-architecture","text":"Architecture overview for AWS deployments of pltf.","title":"AWS Architecture"},{"location":"security/aws/#description","text":"Single-region deployments with networking across three AZs by default (public + private subnets). Public subnets are used for public load balancers; EC2/Databases stay in private subnets (NAT for egress). EKS cluster spans private subnets with managed node groups. Cluster version is configurable ( aws_eks.k8s_version ) and patched by AWS. Public endpoint by default (VPN/private endpoints can be added later). Secrets are encrypted via KMS. Datastores: modules for Postgres (Aurora), Redis (ElastiCache), DocumentDB. Multi-AZ supported; 5-day backup retention for Postgres/DocumentDB. Credentials are generated and passed securely to services. S3: buckets are private by default, encrypted at rest (AES-256); can be made public via inputs. SQS: queues created with dedicated KMS keys for encryption at rest. SNS: topics created with dedicated KMS keys for encryption at rest. IAM: IAM role/user modules with links auto-generate least-privilege policies (S3, SQS, SNS, SES, etc.) and IRSA trusts for Kubernetes services. DNS/SSL: Route53 hosted zone and ACM certificates; validation via Route53; records created to point to the load balancer.","title":"Description"},{"location":"security/aws/#security-overview","text":"End-to-end TLS when using ingress + service mesh (Linkerd optional) and delegated domains. Databases and EC2s in private subnets; only NAT egress. Databases (Postgres/Redis/DocumentDB) encrypted at rest with KMS; connections use SSL. S3 buckets encrypted at rest (AES-256); private by default. SQS/SNS encrypted at rest with per-resource KMS keys. Networking gated by security groups (EKS-managed + module-specific SGs) with minimal port exposure. EKS nodes created with scoped IAM policies; cluster storage (Secrets) encrypted via KMS. K8s service accounts mapped to IAM roles via OIDC (IRSA); no long-lived credentials. No long-lived IAM credentials are created by default; ECR images remain private. 5-day backup retention for Postgres/DocumentDB. Public EKS endpoint by default for simplicity; private/VPN options can be layered later.","title":"Security Overview"},{"location":"security/azure/","text":"Azure Architecture Placeholder. Cover identity (AAD/SP), backend hardening, and network segmentation.","title":"Azure Architecture"},{"location":"security/azure/#azure-architecture","text":"Placeholder. Cover identity (AAD/SP), backend hardening, and network segmentation.","title":"Azure Architecture"},{"location":"security/compliance/","text":"Compliance SOC 2 and PCI considerations for infrastructure deployed by pltf. Overview pltf aims to make SOC 2 and PCI alignment the default for cloud resources it provisions. Compliance is broader than infrastructure; this covers only the cloud layer. Engage a compliance partner for full org-level readiness. Methodology We scan representative environments with Fugue/Regula for SOC2/PCI controls before releases. Findings are fixed when possible; otherwise documented below. Backward-incompatible changes are avoided; new defaults apply to newly created resources. AWS AWS infrastructure can meet SOC2/PCI with the following settings: - S3 buckets: deny non-SSL traffic; enable same_region_replication for backups. - Postgres (Aurora): enable multi_az . Example: modules: - name: db type: aws_postgres multi_az: true - name: s3 type: aws_s3 same_region_replication: true bucket_policy: Version: \"2012-10-17\" Statement: - Sid: denyInsecureTransport Effect: Deny Principal: \"*\" Action: \"s3:*\" Resource: - \"arn:aws:s3:::${parent_name}-${layer_name}/*\" - \"arn:aws:s3:::${parent_name}-${layer_name}\" Condition: Bool: aws:SecureTransport: \"false\" Notes auditors may raise: - Terraform lock DynamoDB table is unencrypted (no customer data, only hashes). - Terraform state bucket logging is not enabled by default (bootstrap ordering). You may manually add logging to the log bucket. - The log bucket does not log itself. GCP Current gaps to full SOC2/PCI: - GKE nodegroup VMs cannot disable block-project-ssh-keys easily. - GKE node disks via KMS key encryption are still limited (beta in TF); will adopt when GA. - Defaults without uniform bucket-level access (GCS state bucket, GCR-backed bucket) to avoid tedious per-user grants; can be manually enabled if desired. Azure Azure can meet SOC2/PCI with an extra user step: - Enable flow logs for the agent pool security group. We continue to monitor provider capabilities and will tighten defaults as features mature.","title":"Compliance"},{"location":"security/compliance/#compliance","text":"SOC 2 and PCI considerations for infrastructure deployed by pltf.","title":"Compliance"},{"location":"security/compliance/#overview","text":"pltf aims to make SOC 2 and PCI alignment the default for cloud resources it provisions. Compliance is broader than infrastructure; this covers only the cloud layer. Engage a compliance partner for full org-level readiness.","title":"Overview"},{"location":"security/compliance/#methodology","text":"We scan representative environments with Fugue/Regula for SOC2/PCI controls before releases. Findings are fixed when possible; otherwise documented below. Backward-incompatible changes are avoided; new defaults apply to newly created resources.","title":"Methodology"},{"location":"security/compliance/#aws","text":"AWS infrastructure can meet SOC2/PCI with the following settings: - S3 buckets: deny non-SSL traffic; enable same_region_replication for backups. - Postgres (Aurora): enable multi_az . Example: modules: - name: db type: aws_postgres multi_az: true - name: s3 type: aws_s3 same_region_replication: true bucket_policy: Version: \"2012-10-17\" Statement: - Sid: denyInsecureTransport Effect: Deny Principal: \"*\" Action: \"s3:*\" Resource: - \"arn:aws:s3:::${parent_name}-${layer_name}/*\" - \"arn:aws:s3:::${parent_name}-${layer_name}\" Condition: Bool: aws:SecureTransport: \"false\" Notes auditors may raise: - Terraform lock DynamoDB table is unencrypted (no customer data, only hashes). - Terraform state bucket logging is not enabled by default (bootstrap ordering). You may manually add logging to the log bucket. - The log bucket does not log itself.","title":"AWS"},{"location":"security/compliance/#gcp","text":"Current gaps to full SOC2/PCI: - GKE nodegroup VMs cannot disable block-project-ssh-keys easily. - GKE node disks via KMS key encryption are still limited (beta in TF); will adopt when GA. - Defaults without uniform bucket-level access (GCS state bucket, GCR-backed bucket) to avoid tedious per-user grants; can be manually enabled if desired.","title":"GCP"},{"location":"security/compliance/#azure","text":"Azure can meet SOC2/PCI with an extra user step: - Enable flow logs for the agent pool security group. We continue to monitor provider capabilities and will tighten defaults as features mature.","title":"Azure"},{"location":"security/gcp/","text":"GCP Architecture Placeholder. Discuss IAM, service accounts, backend security, and network controls.","title":"GCP Architecture"},{"location":"security/gcp/#gcp-architecture","text":"Placeholder. Discuss IAM, service accounts, backend security, and network controls.","title":"GCP Architecture"},{"location":"security/kubernetes/","text":"Kubernetes Architecture Architecture overview for Kubernetes clusters deployed by pltf. Description pltf divides the cluster into namespaces for third-party integrations and for your services. Third-party components are deployed via Helm charts (v3) into their own namespaces; your services are deployed into namespaces derived from the service/layer name. Third-party integrations (common set) Linkerd (service mesh) \u2014 mTLS, traffic control, golden metrics; chosen for simplicity and security. Metrics Server \u2014 HPA metrics (built-in on GKE/Azure; installed on EKS). Cluster Autoscaler \u2014 scales nodes (built-in on GKE/Azure; installed on EKS). Ingress NGINX \u2014 ingress controller routing LB traffic into the cluster. External DNS \u2014 manages DNS records for LBs (not needed on GKE/Azure by default). Datadog (optional) \u2014 metrics/logs/APM via the Datadog K8s integration module. Services (pltf modules) Each service ( aws_k8s_service / gcp_k8s_service ) creates: - Namespace named from the service (layer) name. - Deployment + pods, Service, optional Ingress. - Horizontal Pod Autoscaler (CPU/memory driven). - Service Account wired to cloud IAM via IRSA/Workload Identity; least privilege via links. - ConfigMap/Secrets for app config and credentials (secrets encrypted at rest by the cloud). - Internal DNS of the form <module_name>.<layer_name> for service-to-service calls. Security Overview Linkerd mTLS secures cross-service traffic. Official/Bitnami Helm charts, version-locked; IAM roles scoped to least privilege. Service accounts per service; no extra cluster roles granted by default. IRSA/Workload Identity/OIDC for cloud access; no long-lived credentials in pods. Secrets stored in K8s are encrypted at rest; cloud KMS used by the control plane. plft does not modify aws-auth beyond optional admin_arns configuration. Helm v3 used for all chart deployments.","title":"Kubernetes Architecture"},{"location":"security/kubernetes/#kubernetes-architecture","text":"Architecture overview for Kubernetes clusters deployed by pltf.","title":"Kubernetes Architecture"},{"location":"security/kubernetes/#description","text":"pltf divides the cluster into namespaces for third-party integrations and for your services. Third-party components are deployed via Helm charts (v3) into their own namespaces; your services are deployed into namespaces derived from the service/layer name.","title":"Description"},{"location":"security/kubernetes/#third-party-integrations-common-set","text":"Linkerd (service mesh) \u2014 mTLS, traffic control, golden metrics; chosen for simplicity and security. Metrics Server \u2014 HPA metrics (built-in on GKE/Azure; installed on EKS). Cluster Autoscaler \u2014 scales nodes (built-in on GKE/Azure; installed on EKS). Ingress NGINX \u2014 ingress controller routing LB traffic into the cluster. External DNS \u2014 manages DNS records for LBs (not needed on GKE/Azure by default). Datadog (optional) \u2014 metrics/logs/APM via the Datadog K8s integration module.","title":"Third-party integrations (common set)"},{"location":"security/kubernetes/#services-pltf-modules","text":"Each service ( aws_k8s_service / gcp_k8s_service ) creates: - Namespace named from the service (layer) name. - Deployment + pods, Service, optional Ingress. - Horizontal Pod Autoscaler (CPU/memory driven). - Service Account wired to cloud IAM via IRSA/Workload Identity; least privilege via links. - ConfigMap/Secrets for app config and credentials (secrets encrypted at rest by the cloud). - Internal DNS of the form <module_name>.<layer_name> for service-to-service calls.","title":"Services (pltf modules)"},{"location":"security/kubernetes/#security-overview","text":"Linkerd mTLS secures cross-service traffic. Official/Bitnami Helm charts, version-locked; IAM roles scoped to least privilege. Service accounts per service; no extra cluster roles granted by default. IRSA/Workload Identity/OIDC for cloud access; no long-lived credentials in pods. Secrets stored in K8s are encrypted at rest; cloud KMS used by the control plane. plft does not modify aws-auth beyond optional admin_arns configuration. Helm v3 used for all chart deployments.","title":"Security Overview"}]}